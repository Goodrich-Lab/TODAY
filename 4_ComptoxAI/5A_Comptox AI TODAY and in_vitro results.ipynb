{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddedfe82",
   "metadata": {},
   "source": [
    "# 0. Set up project and read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0eb8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\r\n",
      "Python 3.10.9\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a9d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up \n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = Path.cwd()\n",
    "\n",
    "# Get folder names\n",
    "dir_dat = current_directory.parent.parent.parent / \"0_data\" / \"clean_data\"\n",
    "dir_res = current_directory.parent.parent.parent / \"2_results\"\n",
    "\n",
    "# Read in proteomics metadata\n",
    "# gene_data = pd.read_csv(dir_res / 'Combined_proteomics_scRNAseq_sig_results_073024.csv')\n",
    "gene_data = pd.read_csv(dir_res / 'Combined_proteomics_scRNAseq_sig_fdr_results_073024.csv')\n",
    "\n",
    "# Get names of genes in a list\n",
    "gene_list = gene_data['EntrezGeneSymbol'].tolist()\n",
    "\n",
    "# Combine Genes into str\n",
    "genes = \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in gene_list]) + \"]\"\n",
    "\n",
    "#print(split_genes)\n",
    "#print(split_genes)\n",
    "# type(gene_data)\n",
    "#print(type(prot_metadata.EntrezGeneSymbol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee310eed",
   "metadata": {},
   "source": [
    "Get list of significant genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25c4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_both_human_invitro = gene_data[gene_data['sig_overall_simplified'] == 'Both']['EntrezGeneSymbol'].tolist()\n",
    "genes_human_only = gene_data[gene_data['sig_overall_simplified'] == 'TODAY']['EntrezGeneSymbol'].tolist()\n",
    "genes_in_vitro_only = gene_data[gene_data['sig_overall_simplified'] == 'scRNAseq']['EntrezGeneSymbol'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233008c",
   "metadata": {},
   "source": [
    "# 1. Query ComptoxAI database using Neo4j.\n",
    "Note: to get this to run, you have to have an Neo4j instance of comptoxai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47aa4fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to public Neo4j database at `localhost:7687`...\n",
      "...connection established successfully.\n",
      "Attempting to connect to public Neo4j database at `localhost:7687`...\n",
      "...connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connect to neo4j database\n",
    "import comptox_ai\n",
    "from comptox_ai.db.graph_db import GraphDB\n",
    "db = GraphDB(username=\"cytoscape\", password= \"12345\", hostname=\"localhost:7687\")\n",
    "#db = GraphDB(username=\"neo4j_user\", password=\"12345\", hostname=\"localhost:7687\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96eab8b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create cypher query\n",
    "# Start and end strings\n",
    "start_string = \"MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (node1:Gene) WHERE node1.geneSymbol IN \"\n",
    "\n",
    "end_string = \"WITH collect(id(node1))+collect(c)+collect(d) as nodes CALL apoc.algo.cover(nodes) YIELD rel RETURN  startNode(rel), rel, endNode(rel);\"\n",
    "\n",
    "# Combine the start and end strings with the unique_gene_symbols_list\n",
    "query_string = start_string + \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in gene_list]) + \"] \" + end_string\n",
    "\n",
    "# Run Cypher Query\n",
    "data = db.run_cypher(query_string)\n",
    "#print(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851d678",
   "metadata": {},
   "source": [
    "# 2. Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78d81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network diagram\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a new graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# List mediator proteins\n",
    "\n",
    "# Function to compute the combined 'type' attribute\n",
    "def compute_type(node):\n",
    "    if node.get('commonName') == 'PFNA':\n",
    "        return 'PFAS'\n",
    "    if node.get('commonName') == 'Diabetic Nephropathy':\n",
    "        return 'Disease'\n",
    "    if node.get('geneSymbol') in genes_both_human_invitro:\n",
    "        return 'Gene: Human and in-vitro'\n",
    "    if node.get('geneSymbol') in genes_human_only:\n",
    "        return 'Gene: Human only'\n",
    "    else:\n",
    "        if node.get('geneSymbol') in genes_in_vitro_only:\n",
    "            return 'Gene: In-vitro only'\n",
    "        else:\n",
    "            return 'error'\n",
    "\n",
    "    \n",
    "# Add nodes and edges with combined 'type' attribute\n",
    "for entry in data:\n",
    "    start_node = entry['startNode(rel)']\n",
    "    end_node = entry['endNode(rel)']\n",
    "    rel_type = entry['rel'][1]  # Relationship type is the second item in the tuple\n",
    "    \n",
    "    # Set combined 'type' attribute\n",
    "    start_node['type'] = compute_type(start_node)\n",
    "    end_node['type'] = compute_type(end_node)\n",
    "\n",
    "    # Node identifiers\n",
    "    start_node_id = start_node.get('geneSymbol')  or start_node.get('commonName')\n",
    "    end_node_id = end_node.get('geneSymbol') or end_node.get('xrefUmlsCUI') \n",
    "\n",
    "    # Add nodes with combined 'type' attribute\n",
    "    G.add_node(start_node_id, **start_node)\n",
    "    G.add_node(end_node_id, **end_node)\n",
    "\n",
    "    # Add edge\n",
    "    G.add_edge(start_node_id, end_node_id, relationship=rel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be701b",
   "metadata": {},
   "source": [
    "### Create figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc852ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'grey': 3757, 'blue': 296, 'green': 106, 'red': 1, 'magenta': 1})\n",
      "4161\n",
      "Counter({'grey': 3757, 'blue': 296, 'green': 106, 'red': 1, 'magenta': 1})\n",
      "4161\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Color mapping\n",
    "color_map = {\n",
    "    'PFAS': 'magenta',\n",
    "    'Disease': 'red',\n",
    "    'Gene: Human and in-vitro': 'green',\n",
    "    'Gene: Human only': 'blue', \n",
    "    'Gene: In-vitro only': 'grey'\n",
    "    \n",
    "}\n",
    "\n",
    "# Compute node colors based on 'type' attribute\n",
    "node_colors = [color_map[G.nodes[node]['type']] for node in G]\n",
    "\n",
    "print(Counter(node_colors))\n",
    "print(len(G))\n",
    "#print(len(node_colors))\n",
    "\n",
    "# Draw the graph\n",
    "#plt.figure(figsize=(12, 8))  # Set the figure size\n",
    "#pos = nx.spring_layout(G)  # Layout for the nodes\n",
    "#nx.draw_networkx(G, pos, with_labels=True, node_color=node_colors, node_size=700, edge_color='k', linewidths=1, font_size=10, arrows=True)\n",
    "#nx.draw_networkx_edge_labels(G, pos, edge_labels=nx.get_edge_attributes(G, 'relationship'))\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2ca34",
   "metadata": {},
   "source": [
    "# 3. Check graph for issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95481828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4161\n",
      "Nodes only connected to themselves: []\n",
      "0\n",
      "4161\n",
      "Nodes only connected to themselves: []\n"
     ]
    }
   ],
   "source": [
    "# Check graph after adding metadata\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "print(nx.number_of_isolates(G))\n",
    "print(len(G.nodes))\n",
    "\n",
    "# Examine self loop edges:\n",
    "self_loop_edges = list(nx.selfloop_edges(G))\n",
    "# Extract nodes that have self-loops\n",
    "self_loop_nodes = set(node for node, _ in self_loop_edges)\n",
    "# Identify nodes that only have self-loops and no other connections\n",
    "isolated_self_loop_nodes = [\n",
    "    node for node in self_loop_nodes\n",
    "    if G.degree(node) == 1  # Total degree (in-degree + out-degree) is 1, indicating only a self-loop\n",
    "]\n",
    "print(f\"Nodes only connected to themselves: {isolated_self_loop_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd6382",
   "metadata": {},
   "source": [
    "## 3.a Identify disconnected graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c71d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate weakly connected components\n",
    "weakly_connected_components = nx.weakly_connected_components(G)\n",
    "\n",
    "# Get the size of each component (number of nodes)\n",
    "component_sizes = [len(component) for component in weakly_connected_components]\n",
    "# print(Counter(component_sizes))\n",
    "\n",
    "# Identify genes not in the large componenet -----------\n",
    "\n",
    "# Find all weakly connected components\n",
    "weakly_connected_components = list(nx.weakly_connected_components(G))\n",
    "\n",
    "# Find the largest weakly connected component (by number of nodes)\n",
    "largest_component = max(weakly_connected_components, key=len)\n",
    "\n",
    "# Find all nodes in the graph\n",
    "all_nodes = set(G.nodes())\n",
    "\n",
    "# Find nodes not in the largest component\n",
    "nodes_not_in_largest = all_nodes - largest_component\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = []\n",
    "for node in nodes_not_in_largest:\n",
    "    node_type = G.nodes[node].get('type', 'Unknown')  # Use 'Unknown' if no type is provided\n",
    "    data.append((node, node_type))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Node', 'Type'])\n",
    "\n",
    "# Sort the DataFrame by the 'Type' column\n",
    "df_sorted = df.sort_values(by='Type')\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "#print(df_sorted)\n",
    "\n",
    "# Remove these nodes from the graph\n",
    "G_trim = G.copy()\n",
    "G_trim.remove_nodes_from(nodes_not_in_largest)\n",
    "\n",
    "# print(len(G.nodes))\n",
    "# print(len(G_trim.nodes))\n",
    "# print(len(nodes_not_in_largest))\n",
    "# print(len(G_trim.nodes) == len(G.nodes)-len(nodes_not_in_largest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee47a62",
   "metadata": {},
   "source": [
    "# 4. Analyze communities and paths between PFNA and DKD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fe155",
   "metadata": {},
   "source": [
    "## 4.a. leidenalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc080840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# leidenalg Communities \n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import cairocffi as cairo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "source_node = \"PFNA\"\n",
    "target_node = \"C0011881\"\n",
    "\n",
    "# Find all simple paths from source_node to target_node\n",
    "all_paths = list(nx.shortest_path(G_trim, source=source_node, target=target_node))\n",
    "# print(all_paths)\n",
    "\n",
    "# convert to igraph\n",
    "h = ig.Graph.from_networkx(G_trim)\n",
    "\n",
    "# Identify cluster partitions\n",
    "partition = la.find_partition(h, la.ModularityVertexPartition)\n",
    "print(len(partition))\n",
    "# optimiser = la.Optimiser()\n",
    "# # Optimize partitions\n",
    "# diff = optimiser.optimise_partition(partition, n_iterations=500)\n",
    "# print(len(partition))\n",
    "\n",
    "# print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9b5154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leidenalg community assignments\n",
    "communities1 = partition.membership\n",
    "# Create a concatenated variable that combines groups 9 and above\n",
    "communities2 = [i if i < 9 else 9 for i in communities1]\n",
    "\n",
    "# # Create a mapping from igraph vertices to NetworkX nodes\n",
    "# Assuming nodes in G_trim are labeled from 0 to n-1\n",
    "mapping = {v.index: node for v, node in zip(h.vs, G_trim.nodes)}\n",
    "\n",
    "# Add community1 information to NetworkX graph\n",
    "for idx, community in enumerate(communities1):\n",
    "    nx_node = mapping[idx]\n",
    "    G_trim.nodes[nx_node]['community1'] = community\n",
    "\n",
    "\n",
    "# Add community2 information to NetworkX graph\n",
    "for idx, community in enumerate(communities2):\n",
    "    nx_node = mapping[idx]\n",
    "    G_trim.nodes[nx_node]['community2'] = community\n",
    "\n",
    "# Now G_trim has the community information\n",
    "# Example: Print node and its community\n",
    "# for node in G_trim.nodes(data=True):\n",
    "#     print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aef68",
   "metadata": {},
   "source": [
    "## 4.b. pygenstability (takes a long time and detects way too many communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20de7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pygenstability as pgs\n",
    "# import scipy.sparse as sp\n",
    "# import pygenstability as pgs\n",
    "# from pygenstability import plotting\n",
    "# from pygenstability.pygenstability import evaluate_NVI\n",
    "\n",
    "# A = nx.to_scipy_sparse_matrix(G_trim)\n",
    "# # run markov stability and identify optimal scales\n",
    "# results = pgs.run(\n",
    "#     A,\n",
    "#     min_scale=-1.25,\n",
    "#     max_scale=0.75,\n",
    "#     n_scale=50,\n",
    "#     n_tries=20,\n",
    "#     constructor=\"continuous_combinatorial\",\n",
    "#     n_workers=4\n",
    "# )\n",
    "# \n",
    "# pgs.plot_scan(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0709ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plotting.plot_scan(results, figure_name=None)\n",
    "# plotting.plot_optimal_partitions(G_trim, results, edge_color='0.5', edge_width=0.5, folder='optimal_partitions', ext='.pdf', show=False)\n",
    "# plot matrix\n",
    "# plt.figure()\n",
    "# plt.imshow(A, interpolation=\"nearest\")\n",
    "\n",
    "# print(f\"Number of nodes in G_trim: {len(G_trim.nodes)}\")\n",
    "# print(f\"Length of node_color array: {len(results['community_id'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13f5bb",
   "metadata": {},
   "source": [
    "## 4.c. Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d818a35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no improvement.\n",
      "Partitioned to codelength 3.82274813 + 4.60052872 = 8.423276853 in 350 (342 non-trivial) modules.\n",
      "\n",
      "=> Trial 1/1 finished in 0.079026291s with codelength 8.42327685\n",
      "\n",
      "\n",
      "================================================\n",
      "Summary after 1 trial\n",
      "================================================\n",
      "Best end modular solution in 2 levels:\n",
      "Per level number of modules:         [        350,           0] (sum: 350)\n",
      "Per level number of leaf nodes:      [          0,        4128] (sum: 4128)\n",
      "Per level average child degree:      [        350,     11.7943] (average: 38.2284)\n",
      "Per level codelength for modules:    [3.822748134, 0.000000000] (sum: 3.822748134)\n",
      "Per level codelength for leaf nodes: [0.000000000, 4.600528719] (sum: 4.600528719)\n",
      "Per level codelength total:          [3.822748134, 4.600528719] (sum: 8.423276853)\n",
      "\n",
      "===================================================\n",
      "  Infomap ends at 2024-08-06 09:31:35\n",
      "  (Elapsed time: 0.089764792s)\n",
      "===================================================\n",
      "BiasedMapEquation::init()...\n",
      "=======================================================\n",
      "  Infomap v2.8.0 starts at 2024-08-06 09:31:49\n",
      "  -> Input network: \n",
      "  -> No file output!\n",
      "  -> Configuration: tree\n",
      "                    ftree\n",
      "                    clu\n",
      "                    output = json\n",
      "                    two-level\n",
      "                    flow-model = directed\n",
      "                    directed\n",
      "                    verbose\n",
      "=======================================================\n",
      "  -> Ordinary network input, using the Map Equation for first order network flows\n",
      "Calculating global network flow using flow model 'directed'... \n",
      "  -> Using unrecorded teleportation to links. \n",
      "  -> PageRank calculation done in 53 iterations.\n",
      "\n",
      "  => Sum node flow: 1, sum link flow: 1\n",
      "Build internal network with 4128 nodes and 21820 links...\n",
      "BiasedMapEquation::init()...\n",
      "InfomapBase::init()...\n",
      "BiasedMapEquation::initNetwork()...\n",
      "MapEquation::initNetwork()...\n",
      "  -> One-level codelength: 10.6643265\n",
      "Run Infomap...\n",
      "\n",
      "================================================\n",
      "Trial 1/1 starting at 2024-08-06 09:31:49\n",
      "================================================\n",
      "Trying to find modular structure... \n",
      "\n",
      "Iteration 1:\n",
      "Level 1 (codelength: 10.6643265 + 1.50669506 = 12.17102152): Moving 4128 nodes... done! -> codelength 6.21740244 + 3.0804164 = 9.297818842 in 1013 modules.\n",
      "Level 2 (codelength: 6.21740244 + 3.0804164 = 9.297818842): Moving 1013 nodes... done! -> codelength 4.01296164 + 4.51062286 = 8.523584506 in 372 modules.\n",
      "Level 3 (codelength: 4.01296164 + 4.51062286 = 8.523584506): Moving 372 nodes... done! -> codelength 3.91130561 + 4.60160319 = 8.512908792 in 348 modules.\n",
      "Level 4 (codelength: 3.91130561 + 4.60160319 = 8.512908792): Moving 348 nodes... done! -> codelength 3.91130561 + 4.60160319 = 8.512908792 in 348 modules.\n",
      "-> Restored to codelength 3.91130561 + 4.60160319 = 8.512908792 in 348 modules.\n",
      "\n",
      "Fine tune...  -> moved to codelength 3.91130561 + 4.60160319 = 8.512908792 in 348 existing modules. Try tuning...\n",
      " -> done in 3 effective loops to codelength 3.87637543 + 4.55619227 = 8.432567696 in 355 modules.\n",
      "\n",
      "Iteration 2:\n",
      "Level 2 (codelength: 3.87637543 + 4.55619227 = 8.432567696): Moving 355 nodes... done! -> codelength 3.85246249 + 4.5786549 = 8.431117385 in 349 modules.\n",
      "Level 3 (codelength: 3.85246249 + 4.5786549 = 8.431117385): Moving 349 nodes... done! -> codelength 3.85246249 + 4.5786549 = 8.431117385 in 349 modules.\n",
      "-> Restored to codelength 3.85246249 + 4.5786549 = 8.431117385 in 349 modules.\n",
      "\n",
      "Coarse tune... done in 1 effective loops to codelength 3.85396717 + 4.57627742 = 8.430244592 in 350 modules.\n",
      "\n",
      "Iteration 3:\n",
      "Level 2 (codelength: 3.85396717 + 4.57627742 = 8.430244592): Moving 350 nodes... done! -> codelength 3.85396717 + 4.57627742 = 8.430244592 in 350 modules.\n",
      "-> Restored to codelength 3.85396717 + 4.57627742 = 8.430244592 in 350 modules.\n",
      "\n",
      "Fine tune...  -> moved to codelength 3.85396717 + 4.57627742 = 8.430244592 in 350 existing modules. Try tuning...\n",
      " -> done in 2 effective loops to codelength 3.84519832 + 4.58052963 = 8.425727954 in 351 modules.\n",
      "\n",
      "Iteration 4:\n",
      "Level 2 (codelength: 3.84519832 + 4.58052963 = 8.425727954): Moving 351 nodes... done! -> codelength 3.81940151 + 4.6056691 = 8.425070608 in 349 modules.\n",
      "Level 3 (codelength: 3.81940151 + 4.6056691 = 8.425070608): Moving 349 nodes... done! -> codelength 3.81940151 + 4.6056691 = 8.425070608 in 349 modules.\n",
      "Level 4 (codelength: 3.81940151 + 4.6056691 = 8.425070608): Moving 349 nodes... done! -> codelength 3.81940151 + 4.6056691 = 8.425070608 in 349 modules.\n",
      "-> Restored to codelength 3.81940151 + 4.6056691 = 8.425070608 in 349 modules.\n",
      "\n",
      "Coarse tune... done in 1 effective loops to codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 modules.\n",
      "\n",
      "Iteration 5:\n",
      "Level 2 (codelength: 3.8200759 + 4.60445698 = 8.424532879): Moving 350 nodes... done! -> codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 modules.\n",
      "Level 3 (codelength: 3.8200759 + 4.60445698 = 8.424532879): Moving 350 nodes... done! -> codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 modules.\n",
      "-> Restored to codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 modules.\n",
      "\n",
      "Fine tune...  -> moved to codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 existing modules. Try tuning...\n",
      " -> done in 2 effective loops to codelength 3.82274813 + 4.60052872 = 8.423276853 in 350 modules.\n",
      "\n",
      "Iteration 6:\n",
      "Level 2 (codelength: 3.82274813 + 4.60052872 = 8.423276853): Moving 350 nodes... done! -> codelength 3.82274813 + 4.60052872 = 8.423276853 in 350 modules.\n",
      "-> Restored to codelength 3.82274813 + 4.60052872 = 8.423276853 in 350 modules.\n",
      "\n",
      "Coarse tune... no improvement.\n",
      "Partitioned to codelength 3.82274813 + 4.60052872 = 8.423276853 in 350 (342 non-trivial) modules.\n",
      "\n",
      "=> Trial 1/1 finished in 0.079026291s with codelength 8.42327685\n",
      "\n",
      "\n",
      "================================================\n",
      "Summary after 1 trial\n",
      "================================================\n",
      "Best end modular solution in 2 levels:\n",
      "Per level number of modules:         [        350,           0] (sum: 350)\n",
      "Per level number of leaf nodes:      [          0,        4128] (sum: 4128)\n",
      "Per level average child degree:      [        350,     11.7943] (average: 38.2284)\n",
      "Per level codelength for modules:    [3.822748134, 0.000000000] (sum: 3.822748134)\n",
      "Per level codelength for leaf nodes: [0.000000000, 4.600528719] (sum: 4.600528719)\n",
      "Per level codelength total:          [3.822748134, 4.600528719] (sum: 8.423276853)\n",
      "\n",
      "===================================================\n",
      "  Infomap ends at 2024-08-06 09:31:35\n",
      "  (Elapsed time: 0.089764792s)\n",
      "===================================================\n",
      "BiasedMapEquation::init()...\n",
      "=======================================================\n",
      "  Infomap v2.8.0 starts at 2024-08-06 09:31:49\n",
      "  -> Input network: \n",
      "  -> No file output!\n",
      "  -> Configuration: tree\n",
      "                    ftree\n",
      "                    clu\n",
      "                    output = json\n",
      "                    two-level\n",
      "                    flow-model = directed\n",
      "                    directed\n",
      "                    verbose\n",
      "=======================================================\n",
      "  -> Ordinary network input, using the Map Equation for first order network flows\n",
      "Calculating global network flow using flow model 'directed'... \n",
      "  -> Using unrecorded teleportation to links. \n",
      "  -> PageRank calculation done in 53 iterations.\n",
      "\n",
      "  => Sum node flow: 1, sum link flow: 1\n",
      "Build internal network with 4128 nodes and 21820 links...\n",
      "BiasedMapEquation::init()...\n",
      "InfomapBase::init()...\n",
      "BiasedMapEquation::initNetwork()...\n",
      "MapEquation::initNetwork()...\n",
      "  -> One-level codelength: 10.6643265\n",
      "Run Infomap...\n",
      "\n",
      "================================================\n",
      "Trial 1/1 starting at 2024-08-06 09:31:49\n",
      "================================================\n",
      "Trying to find modular structure... \n",
      "\n",
      "Iteration 1:\n",
      "Level 1 (codelength: 10.6643265 + 1.50669506 = 12.17102152): Moving 4128 nodes... done! -> codelength 6.21740244 + 3.0804164 = 9.297818842 in 1013 modules.\n",
      "Level 2 (codelength: 6.21740244 + 3.0804164 = 9.297818842): Moving 1013 nodes... done! -> codelength 4.01296164 + 4.51062286 = 8.523584506 in 372 modules.\n",
      "Level 3 (codelength: 4.01296164 + 4.51062286 = 8.523584506): Moving 372 nodes... done! -> codelength 3.91130561 + 4.60160319 = 8.512908792 in 348 modules.\n",
      "Level 4 (codelength: 3.91130561 + 4.60160319 = 8.512908792): Moving 348 nodes... done! -> codelength 3.91130561 + 4.60160319 = 8.512908792 in 348 modules.\n",
      "-> Restored to codelength 3.91130561 + 4.60160319 = 8.512908792 in 348 modules.\n",
      "\n",
      "Fine tune...  -> moved to codelength 3.91130561 + 4.60160319 = 8.512908792 in 348 existing modules. Try tuning...\n",
      " -> done in 3 effective loops to codelength 3.87637543 + 4.55619227 = 8.432567696 in 355 modules.\n",
      "\n",
      "Iteration 2:\n",
      "Level 2 (codelength: 3.87637543 + 4.55619227 = 8.432567696): Moving 355 nodes... done! -> codelength 3.85246249 + 4.5786549 = 8.431117385 in 349 modules.\n",
      "Level 3 (codelength: 3.85246249 + 4.5786549 = 8.431117385): Moving 349 nodes... done! -> codelength 3.85246249 + 4.5786549 = 8.431117385 in 349 modules.\n",
      "-> Restored to codelength 3.85246249 + 4.5786549 = 8.431117385 in 349 modules.\n",
      "\n",
      "Coarse tune... done in 1 effective loops to codelength 3.85396717 + 4.57627742 = 8.430244592 in 350 modules.\n",
      "\n",
      "Iteration 3:\n",
      "Level 2 (codelength: 3.85396717 + 4.57627742 = 8.430244592): Moving 350 nodes... done! -> codelength 3.85396717 + 4.57627742 = 8.430244592 in 350 modules.\n",
      "-> Restored to codelength 3.85396717 + 4.57627742 = 8.430244592 in 350 modules.\n",
      "\n",
      "Fine tune...  -> moved to codelength 3.85396717 + 4.57627742 = 8.430244592 in 350 existing modules. Try tuning...\n",
      " -> done in 2 effective loops to codelength 3.84519832 + 4.58052963 = 8.425727954 in 351 modules.\n",
      "\n",
      "Iteration 4:\n",
      "Level 2 (codelength: 3.84519832 + 4.58052963 = 8.425727954): Moving 351 nodes... done! -> codelength 3.81940151 + 4.6056691 = 8.425070608 in 349 modules.\n",
      "Level 3 (codelength: 3.81940151 + 4.6056691 = 8.425070608): Moving 349 nodes... done! -> codelength 3.81940151 + 4.6056691 = 8.425070608 in 349 modules.\n",
      "Level 4 (codelength: 3.81940151 + 4.6056691 = 8.425070608): Moving 349 nodes... done! -> codelength 3.81940151 + 4.6056691 = 8.425070608 in 349 modules.\n",
      "-> Restored to codelength 3.81940151 + 4.6056691 = 8.425070608 in 349 modules.\n",
      "\n",
      "Coarse tune... done in 1 effective loops to codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 modules.\n",
      "\n",
      "Iteration 5:\n",
      "Level 2 (codelength: 3.8200759 + 4.60445698 = 8.424532879): Moving 350 nodes... done! -> codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 modules.\n",
      "Level 3 (codelength: 3.8200759 + 4.60445698 = 8.424532879): Moving 350 nodes... done! -> codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 modules.\n",
      "-> Restored to codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 modules.\n",
      "\n",
      "Fine tune...  -> moved to codelength 3.8200759 + 4.60445698 = 8.424532879 in 350 existing modules. Try tuning...\n",
      " -> done in 2 effective loops to codelength 3.82274813 + 4.60052872 = 8.423276853 in 350 modules.\n",
      "\n",
      "Iteration 6:\n",
      "Level 2 (codelength: 3.82274813 + 4.60052872 = 8.423276853): Moving 350 nodes... done! -> codelength 3.82274813 + 4.60052872 = 8.423276853 in 350 modules.\n",
      "-> Restored to codelength 3.82274813 + 4.60052872 = 8.423276853 in 350 modules.\n",
      "\n",
      "Coarse tune... "
     ]
    }
   ],
   "source": [
    "from infomap import Infomap\n",
    "\n",
    "# Create a mapping from original node labels to integers\n",
    "mapping = {node: i for i, node in enumerate(G_trim.nodes())}\n",
    "reverse_mapping = {i: node for node, i in mapping.items()}\n",
    "\n",
    "# Relabel the graph using the integer mapping\n",
    "G_trim_int = nx.relabel_nodes(G_trim, mapping)\n",
    "\n",
    "# Initialize Infomap\n",
    "infomap = Infomap(\"--two-level --directed --flow-model directed -vvv --tree --ftree --clu -o json \")\n",
    "\n",
    "# Add edges to Infomap\n",
    "for u, v in G_trim_int.edges():\n",
    "    infomap.add_link(u, v)\n",
    "\n",
    "# Run Infomap algorithm\n",
    "infomap.run()\n",
    "\n",
    "# Extract communities\n",
    "communities = {}\n",
    "for node, module in infomap.get_modules().items():\n",
    "    original_node = reverse_mapping[node]\n",
    "    if module not in communities:\n",
    "        communities[module] = []\n",
    "    communities[module].append(original_node)\n",
    "\n",
    "# Print the detected communities\n",
    "# for module, nodes in communities.items():\n",
    "#     print(f\"Community {module}: {nodes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1af4ee17",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'infomap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43mhelp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfomap\u001b[49m\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'infomap'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d13895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize the communities\n",
    "\n",
    "# Assign a color to each community\n",
    "colors = [infomap.get_modules()[mapping[node]] for node in G_trim.nodes()]\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_trim)\n",
    "nx.draw_networkx_nodes(G_trim, pos, node_color=colors, cmap=plt.cm.tab20, node_size=1)\n",
    "nx.draw_networkx_edges(G_trim, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIDNT WORK BECAUSE THERE IS NO VARIABILITY IN THE CAPACITY\n",
    "# from networkx.algorithms.flow import maximum_flow\n",
    "\n",
    "# # Assume G is your directed graph (DiGraph)\n",
    "# source_node = \"PFNA\"\n",
    "# target_node = \"C0011881\"\n",
    "\n",
    "# # Set a default capacity for all edges if not already present\n",
    "# for u, v in G.edges():\n",
    "#     if 'capacity' not in G[u][v]:\n",
    "#         G[u][v]['capacity'] = 10  # Set to 1 or any reasonable default value\n",
    "\n",
    "# # Calculate the original maximum flow\n",
    "# original_flow_value, original_flow_dict = maximum_flow(G, source_node, target_node)\n",
    "# print(f\"Original maximum flow from {source_node} to {target_node}: {original_flow_value}\")\n",
    "\n",
    "# # Dictionary to store the impact of each node on the maximum flow\n",
    "# node_impact = {}\n",
    "\n",
    "# # Perform leave-one-out analysis\n",
    "# for node in G.nodes:\n",
    "#     if node in [source_node, target_node]:\n",
    "#         continue  # Skip source and target nodes\n",
    "\n",
    "#     # Create a copy of the graph and remove the current node\n",
    "#     G_temp = G.copy()\n",
    "#     G_temp.remove_node(node)\n",
    "\n",
    "#     # Recalculate the maximum flow\n",
    "#     flow_value, flow_dict = maximum_flow(G_temp, source_node, target_node)\n",
    "\n",
    "#     # Calculate the difference in maximum flow\n",
    "#     flow_difference = original_flow_value - flow_value\n",
    "#     node_impact[node] = flow_difference\n",
    "\n",
    "# # Identify the nodes with the most significant impact on flow\n",
    "# most_impactful_nodes = sorted(node_impact.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Display the nodes and their impact\n",
    "# print(\"Node impact on maximum flow (sorted):\")\n",
    "# for node, impact in most_impactful_nodes:\n",
    "#     print(f\"Node: {node}, Impact: {impact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2197b",
   "metadata": {},
   "source": [
    "# 5. Node2vec analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780241a0",
   "metadata": {},
   "source": [
    "## 5.a. Node2vec (undirected graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes ~1 minute\n",
    "# from node2vec import Node2Vec\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Generate Node2Vec model\n",
    "# node2vec = Node2Vec(G_trim, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "# model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# # Extract embeddings for each node\n",
    "# embeddings = model.wv\n",
    "# node_ids = list(G_trim.nodes)\n",
    "# node_embeddings = [embeddings[node] for node in node_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4f57d",
   "metadata": {},
   "source": [
    "## 5.b. TSNE/UMAP on node2vec results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run time ~ 10 seconds\n",
    "# Perform t-SNE to reduce dimensionality to 2D for visualization\n",
    "node_embeddings = np.array(node_embeddings)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_embeddings_2d = tsne.fit_transform(node_embeddings)\n",
    "\n",
    "# Note- JG tried umap, but it didn't look good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de2923",
   "metadata": {},
   "source": [
    "## 5.c. Cluster TSNE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060dab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run time ~ 20 seconds\n",
    "from sklearn.cluster import KMeans, DBSCAN, HDBSCAN, AgglomerativeClustering, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Perform K-Means clustering on the original node embeddings\n",
    "n_clusters = 5  # Set the number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_clusters = kmeans.fit_predict(node_embeddings)\n",
    "\n",
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "dbscan_clusters = dbscan.fit_predict(node_embeddings)\n",
    "\n",
    "# HDBSCAN clustering\n",
    "hdbscan = HDBSCAN(min_cluster_size= 8)\n",
    "hdbscan_clusters = hdbscan.fit_predict(node_embeddings)\n",
    "\n",
    "# Agglomerative Clustering\n",
    "agglomerativeclustering = AgglomerativeClustering(n_clusters= 5)\n",
    "agglomerative_clusters = agglomerativeclustering.fit_predict(node_embeddings)\n",
    "\n",
    "# Birch Clustering\n",
    "birch = Birch(n_clusters= 4)\n",
    "birch_clusters = birch.fit_predict(node_embeddings)\n",
    "\n",
    "# Gaussian Mixture Model clustering\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm_clusters = gmm.fit_predict(node_embeddings)\n",
    "\n",
    "\n",
    "print('K-means: ' + str(Counter(kmeans_clusters)))\n",
    "print('dbscan: '  + str(Counter(dbscan_clusters)))\n",
    "print('hdbscan: ' + str(Counter(hdbscan_clusters)))\n",
    "print('agglomerativeclustering: ' + str(Counter(agglomerative_clusters)))\n",
    "print('Birch: ' + str(Counter(birch_clusters)))\n",
    "print('GaussianMixture: ' + str(Counter(gmm_clusters)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072cc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame linking node names with their 2D embeddings\n",
    "df_embeddings = pd.DataFrame(tsne_embeddings_2d, columns=['tsne_dim1', 'tsne_dim2'])\n",
    "# umap_embeddings = pd.DataFrame(umap_embeddings_2d, columns=['umap_dim1', 'umap_dim2'])\n",
    "# df_embeddings = pd.merge(umap_embeddings, tsne_embeddings,  left_index=True, right_index=True)\n",
    "df_embeddings['gene'] = node_ids\n",
    "\n",
    "# Reorder columns to have 'Node' first\n",
    "df_embeddings = df_embeddings[['gene', 'tsne_dim1', 'tsne_dim2']] # , 'umap_dim1', 'umap_dim2'\n",
    "\n",
    "# Merge with clusters \n",
    "df_embeddings['kmeans_clusters']        = kmeans_clusters\n",
    "df_embeddings['dbscan_clusters']        = dbscan_clusters\n",
    "df_embeddings['hdbscan_clusters']       = hdbscan_clusters\n",
    "df_embeddings['agglomerative_clusters'] = agglomerative_clusters\n",
    "df_embeddings['birch']                  = birch_clusters\n",
    "df_embeddings['gmm_clusters']           = gmm_clusters\n",
    "\n",
    "# Before merge: record the number of rows in each dataframe\n",
    "rows_before = {'df_embeddings': len(df_embeddings), 'gene_data': len(gene_data)}\n",
    "\n",
    "# Merging with gene_data metadata, by 'gene' and 'EntrezGeneSymbol' columns\n",
    "df_merged = pd.merge(df_embeddings, gene_data, how='left', left_on='gene', right_on='EntrezGeneSymbol')\n",
    "\n",
    "\n",
    "# After merge: record the number of rows in the merged dataframe\n",
    "rows_after = len(df_merged)\n",
    "\n",
    "# Print the merge details\n",
    "print(f\"Number of rows in df_embeddings before merge: {rows_before['df_embeddings']}\")\n",
    "print(f\"Number of rows in gene_data before merge: {rows_before['gene_data']}\")\n",
    "print(f\"Number of rows in df_merged after merge: {rows_after}\")\n",
    "print(f\"Number of rows matched: {rows_after}\")\n",
    "print(f\"Number of rows lost from df_embeddings: {rows_before['df_embeddings'] - rows_after}\")\n",
    "print(f\"Number of rows lost from original gene_data: {rows_before['gene_data'] - rows_after}\")\n",
    "# print(df_merged.columns)\n",
    "\n",
    "df_merged.to_csv(dir_res / 'network_graph_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd00edd",
   "metadata": {},
   "source": [
    "## 5.c. Plot TSNE Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign numeric codes to categories for coloring\n",
    "categories      = pd.Categorical(df_merged['sig_overall'])\n",
    "category_colors = pd.Categorical(df_merged['sig_overall']).codes\n",
    "\n",
    "# List of cluster columns to plot\n",
    "cluster_columns = [\n",
    "    'kmeans_clusters', \n",
    "    'dbscan_clusters', \n",
    "    'hdbscan_clusters', \n",
    "    'agglomerative_clusters', \n",
    "    'birch', \n",
    "    'gmm_clusters'\n",
    "]\n",
    "\n",
    "# Set up the figure and axes for the subplots\n",
    "fig, axes = plt.subplots(1, 6, figsize=(24, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Iterate over the cluster columns and axes to create scatter plots\n",
    "for ax, column in zip(axes, cluster_columns):\n",
    "    scatter = ax.scatter(df_merged['tsne_dim1'], df_merged['tsne_dim2'], c=df_merged[column], cmap='viridis', alpha=0.6, s=10)\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('t-SNE Dimension 1')\n",
    "    ax.set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Add a color bar to the right of the plots\n",
    "# cbar = fig.colorbar(scatter, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "# cbar.set_label('Cluster Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2812e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_columns = [\n",
    "    'kmeans_clusters', \n",
    "    'dbscan_clusters', \n",
    "    'hdbscan_clusters', \n",
    "    'agglomerative_clusters', \n",
    "    'birch', \n",
    "    'gmm_clusters'\n",
    "]\n",
    "\n",
    "# Cross tabs\n",
    "cross_table = pd.crosstab(df_merged['sig_overall'], df_merged['birch'])\n",
    "# Convert counts to percentages\n",
    "# cross_table_percent = cross_table.div(cross_table.sum(axis=1), axis=0) * 100\n",
    "\n",
    "cross_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52280e8",
   "metadata": {},
   "source": [
    "# 6. Save Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81caba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge G_trim and df_merged \n",
    "temp_df = df_merged.copy()\n",
    "G_fin = G_trim.copy()\n",
    "\n",
    "# Set the 'gene' column as the index for easy lookup\n",
    "df_merged_fin = temp_df.loc[:, 'tsne_dim1':'gmm_clusters']\n",
    "df_merged_fin['gene'] = temp_df['EntrezGeneSymbol']\n",
    "# df_merged_fin['gene'] = temp_df['gene']\n",
    "df_merged_fin.set_index('gene', inplace=True)\n",
    "\n",
    "# Iterate through the nodes in G_trim and add attributes from df_merged\n",
    "for node in G_fin.nodes():\n",
    "    if node in df_merged.index:\n",
    "        for attr in df_merged.columns:\n",
    "            G_trim.nodes[node][attr] = df_merged.loc[node, attr]\n",
    "\n",
    "# Example: Print node and its attributes\n",
    "# for node in G_fin.nodes(data=True):\n",
    "#     print(node)\n",
    "\n",
    "# # Save file\n",
    "# file_path = dir_res / 'ComptoxAI' / 'PFAS_prot_in_vitro_sig_fdr_trimmed_073024.graphml'\n",
    "file_path = dir_res / 'ComptoxAI' / 'PFAS_prot_in_vitro_sig_073024.graphml'\n",
    "\n",
    "# Write the graph to a GraphML file\n",
    "nx.write_graphml(G_fin, file_path)\n",
    "print(f\"Network saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign numeric codes to categories for coloring\n",
    "categories      = pd.Categorical(df_merged['sig_overall'])\n",
    "category_colors = pd.Categorical(df_merged['sig_overall']).codes\n",
    "\n",
    "# List of cluster columns to plot\n",
    "cluster_columns = [\n",
    "    'kmeans_clusters', \n",
    "    'dbscan_clusters', \n",
    "    'hdbscan_clusters', \n",
    "    'agglomerative_clusters', \n",
    "    'birch', \n",
    "    'gmm_clusters'\n",
    "]\n",
    "\n",
    "\n",
    "# Set up the figure and axes for the subplots\n",
    "fig, axes = plt.subplots(1, 6, figsize=(24, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Iterate over the cluster columns and axes to create scatter plots\n",
    "for ax, column in zip(axes, cluster_columns):\n",
    "    scatter = ax.scatter(df_merged['tsne_dim1'], df_merged['tsne_dim2'], c=df_merged[column], cmap='viridis', alpha=0.6, s=10)\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('t-SNE Dimension 1')\n",
    "    ax.set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Add a color bar to the right of the plots\n",
    "# cbar = fig.colorbar(scatter, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "# cbar.set_label('Cluster Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021dc1a",
   "metadata": {},
   "source": [
    "# 6. GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e05b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph needs to have the same attributes for this algorithm\n",
    "def get_all_attributes(G_trim):\n",
    "    node_attributes = set()\n",
    "    edge_attributes = set()\n",
    "\n",
    "    # Collect node attributes\n",
    "    for node, attr in G_trim.nodes(data=True):\n",
    "        node_attributes.update(attr.keys())\n",
    "\n",
    "    # Collect edge attributes\n",
    "    for u, v, attr in G_trim.edges(data=True):\n",
    "        edge_attributes.update(attr.keys())\n",
    "\n",
    "    return node_attributes, edge_attributes\n",
    "\n",
    "# Get unique attributes\n",
    "node_attrs, edge_attrs = get_all_attributes(G)\n",
    "print(f\"Unique node attributes: {node_attrs}\")\n",
    "print(f\"Unique edge attributes: {edge_attrs}\")\n",
    "\n",
    "# Desired attributes to keep\n",
    "desired_attributes = {'commonName', 'type', 'typeOfGene'}\n",
    "\n",
    "# Make copy of data\n",
    "G_reduced = G_trim.copy()\n",
    "\n",
    "# Function to filter node attributes\n",
    "def filter_node_attributes(G_reduced, desired_attributes):\n",
    "    for node in G_reduced.nodes:\n",
    "        current_attributes = G_reduced.nodes[node]\n",
    "        filtered_attributes = {k: v for k, v in current_attributes.items() if k in desired_attributes}\n",
    "        G_reduced.nodes[node].clear()\n",
    "        G_reduced.nodes[node].update(filtered_attributes)\n",
    "\n",
    "# # Apply the function to filter node attributes\n",
    "filter_node_attributes(G_reduced, desired_attributes)\n",
    "\n",
    "# Get unique attributes\n",
    "node_attrs, edge_attrs = get_all_attributes(G_reduced)\n",
    "print(f\"Filtered unique node attributes: {node_attrs}\")\n",
    "print(f\"Filtered unique edge attributes: {edge_attrs}\")\n",
    "\n",
    "# Set values for nodes\n",
    "G_reduced.nodes['C0011881']['typeOfGene'] = 'disease' \n",
    "G_reduced.nodes['PFNA']['typeOfGene'] = 'Chemical'  \n",
    "\n",
    "# Check which nodes are missing the \"typeOfGene\" attribute\n",
    "missing_typeOfGene_nodes = [node for node, attr in G_reduced.nodes(data=True) if 'typeOfGene' not in attr]\n",
    "print(f\"Number of missing typeOfGene: {len(missing_typeOfGene_nodes)}\")\n",
    "\n",
    "# Check which nodes are missing the \"typeOfGene\" attribute\n",
    "missing_typeOfGene_nodes = [node for node, attr in G_reduced.nodes(data=True) if 'type' not in attr]\n",
    "print(f\"Number of missing type: {len(missing_typeOfGene_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Example graph data (this should be replaced with your actual graph data)\n",
    "# Assume G is a networkx directed graph with node features and labels\n",
    "\n",
    "# Convert NetworkX graph to PyTorch Geometric Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Create PyTorch Geometric data from NetworkX graph\n",
    "data = from_networkx(G_reduced)\n",
    "\n",
    "# Ensure that data.x (node features) and data.y (labels) are present\n",
    "# Here, we assume each node has a feature vector and a label\n",
    "\n",
    "# Example of adding random node features and labels for demonstration\n",
    "import numpy as np\n",
    "num_nodes = data.num_nodes\n",
    "num_features = 16  # Example feature size\n",
    "num_classes = 3    # Example number of classes\n",
    "\n",
    "data.x = torch.tensor(np.random.randn(num_nodes, num_features), dtype=torch.float)\n",
    "data.y = torch.tensor(np.random.randint(0, num_classes, num_nodes), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5ff8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f3f42a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.a. (OLD) Identify all nodes linking PFNA, genes, and the outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f248f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cypher query\n",
    "# Start and end strings\n",
    "start_string = \"MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (end:Gene) WHERE end.geneSymbol IN \"\n",
    "\n",
    "middle_string = \"CALL apoc.path.spanningTree(d, {relationshipFilter: '<GENEASSOCIATESWITHDISEASE|GENEINTERACTSWITHGENE', minLevel: 1, maxLevel: 5, endNodes: end}) YIELD path WITH nodes(path) as n RETURN n UNION MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (end:Gene) WHERE end.geneSymbol IN\"\n",
    "\n",
    "end_string = \"CALL apoc.path.spanningTree(c, {relationshipFilter: 'CHEMICALINCREASESEXPRESSION>|CHEMICALDECREASESEXPRESSION>|GENEINTERACTSWITHGENE>', minLevel: 1, maxLevel: 7, endNodes: end}) YIELD path WITH nodes(path) as n RETURN n;\"\n",
    "\n",
    "# Combine the start and end strings with the unique_gene_symbols_list\n",
    "first_query = start_string + Entrez_Genes + middle_string + Entrez_Genes + end_string\n",
    "\n",
    "# Run first query to identify all nodes\n",
    "nodes = db.run_cypher(first_query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlist everything and get a list of separate dictionaries\n",
    "flattened_list = [inner_dict for outer_dict in nodes for inner_dict in outer_dict['n']]\n",
    "\n",
    "# Remove duplicates by converting to a set and back to a list\n",
    "unique_list = list({tuple(item.items()) for item in flattened_list})\n",
    "\n",
    "# Convert the list of tuples back to dictionaries\n",
    "unique_list = [dict(item) for item in unique_list]\n",
    "\n",
    "print('The number of unique nodes, including the PFAS and disease node, is ' +  str(len(unique_list)))\n",
    "# Look at first few elements of list\n",
    "# pprint(unique_list[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract geneSymbol\n",
    "# Create a set to store unique geneSymbol values\n",
    "unique_gene_symbols = set()\n",
    "\n",
    "# Iterate through the list and collect unique geneSymbol values\n",
    "for dictionary in flattened_list:\n",
    "    if 'geneSymbol' in dictionary:\n",
    "        unique_gene_symbols.add(dictionary['geneSymbol'])\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "unique_gene_symbols_list = list(unique_gene_symbols)\n",
    "\n",
    "# Now, 'unique_gene_symbols_list' contains all unique geneSymbol values\n",
    "\n",
    "print('The number of unique genes is ' +  str(len(unique_gene_symbols_list)) + \", and \" + str(len(set(unique_gene_symbols_list) & set(Entrez_Gene_Symbols))) + \" of these genes are from the original data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f382b139",
   "metadata": {},
   "source": [
    "## 1.b. (OLD) With all nodes, identify all of the relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c99959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cypher query\n",
    "# Start and end strings\n",
    "start_string = \"MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (node1:Gene) WHERE node1.geneSymbol IN \"\n",
    "\n",
    "end_string = \"WITH collect(id(node1))+collect(c)+collect(d) as nodes CALL apoc.algo.cover(nodes) YIELD rel RETURN  startNode(rel), rel, endNode(rel);\"\n",
    "\n",
    "# Combine the start and end strings with the unique_gene_symbols_list\n",
    "query_string = start_string + \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in unique_gene_symbols_list]) + \"]\" + end_string\n",
    "\n",
    "# Run Cypher Query\n",
    "data = db.run_cypher(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(type(data))\n",
    "print(type(data[1]))\n",
    "#pprint(data[9])\n",
    " \n",
    "# Find elements with 'CHEMICALINCREASESEXPRESSION' in the 'rel' part\n",
    "filtered_elements = [element for element in data if any('CHEMICALINCREASESEXPRESSION' in item for item in element['rel'])]\n",
    "# Print the filtered elements\n",
    "#for idx, element in enumerate(filtered_elements):\n",
    "#    print(f\"Element {idx + 1}:\")\n",
    "#    print(element)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
