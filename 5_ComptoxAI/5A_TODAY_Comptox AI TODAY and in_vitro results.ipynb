{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddedfe82",
   "metadata": {},
   "source": [
    "# 0. Set up project and read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0eb8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34a9d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up \n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = Path.cwd()\n",
    "\n",
    "# Get folder names\n",
    "dir_dat = current_directory.parent.parent.parent / \"0_data\" / \"clean_data\"\n",
    "dir_res = current_directory.parent.parent.parent / \"2_results\"\n",
    "\n",
    "# Read in proteomics metadata\n",
    "# gene_data = pd.read_csv(dir_res / 'Combined_proteomics_scRNAseq_sig_results_073024.csv')\n",
    "gene_data = pd.read_csv(dir_res / 'Combined_proteomics_scRNAseq_sig_fdr_results_090924.csv')\n",
    "\n",
    "# Get names of genes in a list\n",
    "gene_list = gene_data['EntrezGeneSymbol'].tolist()\n",
    "\n",
    "# Combine Genes into str\n",
    "genes = \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in gene_list]) + \"]\"\n",
    "\n",
    "overlapping_gene_list = [\"TMEM87B\", \"THOC1\", \"SPRED1\", \"MFN1\"]\n",
    "#print(split_genes)\n",
    "#print(split_genes)\n",
    "# type(gene_data)\n",
    "#print(type(prot_metadata.EntrezGeneSymbol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee310eed",
   "metadata": {},
   "source": [
    "Get list of significant genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f25c4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_both_human_invitro = gene_data[gene_data['sig_overall_simplified'] == 'Both']['EntrezGeneSymbol'].tolist()\n",
    "genes_human_only = gene_data[gene_data['sig_overall_simplified'] == 'TODAY']['EntrezGeneSymbol'].tolist()\n",
    "genes_in_vitro_only = gene_data[gene_data['sig_overall_simplified'] == 'scRNAseq']['EntrezGeneSymbol'].tolist()\n",
    "#print(genes_both_human_invitro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233008c",
   "metadata": {},
   "source": [
    "# 1. Query ComptoxAI database using Neo4j.\n",
    "Note: to get this to run, you have to have an Neo4j instance of comptoxai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47aa4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to neo4j database\n",
    "import comptox_ai\n",
    "from comptox_ai.db.graph_db import GraphDB\n",
    "db = GraphDB(username=\"cytoscape\", password= \"12345\", hostname=\"localhost:7687\")\n",
    "#db = GraphDB(username=\"neo4j_user\", password=\"12345\", hostname=\"localhost:7687\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96eab8b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create cypher query\n",
    "# Start and end strings\n",
    "start_string = \"MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (node1:Gene) WHERE node1.geneSymbol IN \"\n",
    "\n",
    "end_string = \"WITH collect(id(node1))+collect(c)+collect(d) as nodes CALL apoc.algo.cover(nodes) YIELD rel RETURN  startNode(rel), rel, endNode(rel);\"\n",
    "\n",
    "# Combine the start and end strings with the unique_gene_symbols_list\n",
    "query_string = start_string + \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in gene_list]) + \"] \" + end_string\n",
    "\n",
    "# Run Cypher Query\n",
    "data = db.run_cypher(query_string)\n",
    "# print(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851d678",
   "metadata": {},
   "source": [
    "# 2. Create Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34e664",
   "metadata": {},
   "source": [
    "## 2.a. Create Base Graph from ComptoxAI query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b78d81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network diagram\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a new graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# List mediator proteins\n",
    "\n",
    "# Function to compute the combined 'type' attribute\n",
    "def compute_type(node):\n",
    "    if node.get('commonName') == 'PFNA':\n",
    "        return 'PFAS'\n",
    "    if node.get('commonName') == 'Diabetic Nephropathy':\n",
    "        return 'Disease'\n",
    "    if node.get('geneSymbol') in genes_both_human_invitro:\n",
    "        return 'Gene: Human and in-vitro'\n",
    "    if node.get('geneSymbol') in genes_human_only:\n",
    "        return 'Gene: Human only'\n",
    "    if node.get('geneSymbol') in genes_in_vitro_only:\n",
    "        return 'Gene: In-vitro only'\n",
    "    else:\n",
    "        return 'Gene: from ComptoxAI'\n",
    "\n",
    "    \n",
    "# Add nodes and edges with combined 'type' attribute\n",
    "for entry in data:\n",
    "    start_node = entry['startNode(rel)']\n",
    "    end_node = entry['endNode(rel)']\n",
    "    rel_type = entry['rel'][1]  # Relationship type is the second item in the tuple\n",
    "    \n",
    "    # Set combined 'type' attribute\n",
    "    start_node['type'] = compute_type(start_node)\n",
    "    end_node['type'] = compute_type(end_node)\n",
    "\n",
    "    # Node identifiers\n",
    "    start_node_id = start_node.get('geneSymbol')  or start_node.get('commonName')\n",
    "    end_node_id = end_node.get('geneSymbol') or end_node.get('xrefUmlsCUI') \n",
    "\n",
    "    # Add nodes with combined 'type' attribute\n",
    "    G.add_node(start_node_id, **start_node)\n",
    "    G.add_node(end_node_id, **end_node)\n",
    "\n",
    "    # Add edge\n",
    "    G.add_edge(start_node_id, end_node_id, relationship=rel_type, weight = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cab63",
   "metadata": {},
   "source": [
    "## 2.b Add PFAS and scRNAseq edges- NO LONGER USED BECAUSE THERE WOULD BE TOO MANY EDGES BETWEEN PFAS AND GENES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dbc0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# gene_data\n",
    "\n",
    "# unique_sc_data = list(Counter(sc_data.gene).keys())\n",
    "# unique_sc_data = pd.DataFrame(unique_sc_data, columns= ['gene'])\n",
    "\n",
    "# # Create a new column to store whether the edge exists or not\n",
    "# unique_sc_data['edge_exists'] = False\n",
    "\n",
    "# # Check if edge exists\n",
    "# for idx, row in unique_sc_data.iterrows():\n",
    "#     sc_gene = row['gene']  # The SNP node\n",
    "#     exposure = chemical_common_name  # The name of the PFAS node (which is already be in the network)\n",
    "\n",
    "#     # Check if there is an edge between chemical_common_name and sc_gene\n",
    "#     if G.has_edge(exposure, sc_gene):\n",
    "#         unique_sc_data.at[idx, 'edge_exists'] = True\n",
    "\n",
    "\n",
    "# # Count how many True values there are in the 'edge_exists' column\n",
    "# true_count = unique_sc_data['edge_exists'].sum()\n",
    "\n",
    "# # Create a summary table\n",
    "# summary_table = pd.DataFrame({\n",
    "#     'Total Genes': [len(unique_sc_data)],\n",
    "#     'Edges Found (True)': [true_count],\n",
    "#     'Edges Not Found (False)': [len(unique_sc_data) - true_count]\n",
    "# })\n",
    "\n",
    "# # Display the summary table\n",
    "# print(summary_table)\n",
    "\n",
    "# # Add edge between exposure and scRNAseq genes\n",
    "# # Add or modify edge between exposure and scRNAseq genes\n",
    "# for idx, row in unique_sc_data.iterrows():\n",
    "#     sc_gene = row['gene']  # The gene node\n",
    "#     exposure = chemical_common_name  # The name of the PFAS node\n",
    "\n",
    "#     # Check if the gene is in the graph\n",
    "#     if sc_gene not in G.nodes:\n",
    "#         print(f\"Gene {sc_gene} is not in the graph!\")\n",
    "#         continue  # Skip this gene if it isn't present\n",
    "\n",
    "#     # Check if there is already an edge between exposure and sc_gene\n",
    "#     if G.has_edge(exposure, sc_gene):\n",
    "#         # If the edge exists, modify the relationship and the weight\n",
    "#         G[exposure][sc_gene]['relationship'] = 'ComptoxAI and Organoid association'\n",
    "#         G[exposure][sc_gene]['weight'] = 2\n",
    "#         print(f\"Modified edge {idx}: exposure={exposure}, sc_gene={sc_gene}, relationship='ComptoxAI and Organoid association', weight=2\")\n",
    "#     else:\n",
    "#         # If the edge doesn't exist, add it with a different relationship and weight\n",
    "#         G.add_edge(exposure, sc_gene, relationship='Organoid association', weight=1)\n",
    "#         print(f\"Added new edge {idx}: exposure={exposure}, sc_gene={sc_gene}, relationship='Organoid association', weight=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be701b",
   "metadata": {},
   "source": [
    "### Create figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc852ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'grey': 3901, 'blue': 290, 'green': 111, 'red': 1, 'magenta': 1})\n",
      "4304\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Color mapping\n",
    "color_map = {\n",
    "    'PFAS': 'magenta',\n",
    "    'Disease': 'red',\n",
    "    'Gene: Human and in-vitro': 'green',\n",
    "    'Gene: Human only': 'blue', \n",
    "    'Gene: In-vitro only': 'grey'\n",
    "    \n",
    "}\n",
    "\n",
    "# Compute node colors based on 'type' attribute\n",
    "node_colors = [color_map[G.nodes[node]['type']] for node in G]\n",
    "\n",
    "print(Counter(node_colors))\n",
    "print(len(G))\n",
    "#print(len(node_colors))\n",
    "\n",
    "# Draw the graph\n",
    "#plt.figure(figsize=(12, 8))  # Set the figure size\n",
    "#pos = nx.spring_layout(G)  # Layout for the nodes\n",
    "#nx.draw_networkx(G, pos, with_labels=True, node_color=node_colors, node_size=700, edge_color='k', linewidths=1, font_size=10, arrows=True)\n",
    "#nx.draw_networkx_edge_labels(G, pos, edge_labels=nx.get_edge_attributes(G, 'relationship'))\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2ca34",
   "metadata": {},
   "source": [
    "# 3. Check graph for issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95481828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4304\n",
      "Nodes only connected to themselves: []\n"
     ]
    }
   ],
   "source": [
    "# Check graph after adding metadata\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "print(nx.number_of_isolates(G))\n",
    "print(len(G.nodes))\n",
    "\n",
    "# Examine self loop edges:\n",
    "self_loop_edges = list(nx.selfloop_edges(G))\n",
    "# Extract nodes that have self-loops\n",
    "self_loop_nodes = set(node for node, _ in self_loop_edges)\n",
    "# Identify nodes that only have self-loops and no other connections\n",
    "isolated_self_loop_nodes = [\n",
    "    node for node in self_loop_nodes\n",
    "    if G.degree(node) == 1  # Total degree (in-degree + out-degree) is 1, indicating only a self-loop\n",
    "]\n",
    "print(f\"Nodes only connected to themselves: {isolated_self_loop_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd6382",
   "metadata": {},
   "source": [
    "## 3.a Identify disconnected graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c71d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate weakly connected components\n",
    "weakly_connected_components = nx.weakly_connected_components(G)\n",
    "\n",
    "# Get the size of each component (number of nodes)\n",
    "component_sizes = [len(component) for component in weakly_connected_components]\n",
    "# print(Counter(component_sizes))\n",
    "\n",
    "# Identify genes not in the large componenet -----------\n",
    "\n",
    "# Find all weakly connected components\n",
    "weakly_connected_components = list(nx.weakly_connected_components(G))\n",
    "\n",
    "# Find the largest weakly connected component (by number of nodes)\n",
    "largest_component = max(weakly_connected_components, key=len)\n",
    "\n",
    "# Find all nodes in the graph\n",
    "all_nodes = set(G.nodes())\n",
    "\n",
    "# Find nodes not in the largest component\n",
    "nodes_not_in_largest = all_nodes - largest_component\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = []\n",
    "for node in nodes_not_in_largest:\n",
    "    node_type = G.nodes[node].get('type', 'Unknown')  # Use 'Unknown' if no type is provided\n",
    "    data.append((node, node_type))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Node', 'Type'])\n",
    "\n",
    "# Sort the DataFrame by the 'Type' column\n",
    "df_sorted = df.sort_values(by='Type')\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "#print(df_sorted)\n",
    "\n",
    "# Remove these nodes from the graph\n",
    "G_trim = G.copy()\n",
    "G_trim.remove_nodes_from(nodes_not_in_largest)\n",
    "\n",
    "# print(len(G.nodes))\n",
    "# print(len(G_trim.nodes))\n",
    "# print(len(nodes_not_in_largest))\n",
    "# print(len(G_trim.nodes) == len(G.nodes)-len(nodes_not_in_largest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee47a62",
   "metadata": {},
   "source": [
    "# 4. Analyze communities and paths between PFNA and DKD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fe155",
   "metadata": {},
   "source": [
    "## 4.a. leidenalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc080840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# leidenalg Communities \n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import cairocffi as cairo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "source_node = \"PFNA\"\n",
    "target_node = \"C0011881\"\n",
    "\n",
    "# Find all simple paths from source_node to target_node\n",
    "all_paths = list(nx.shortest_path(G_trim, source=source_node, target=target_node))\n",
    "# print(all_paths)\n",
    "\n",
    "# convert to igraph\n",
    "h = ig.Graph.from_networkx(G_trim)\n",
    "\n",
    "# Identify cluster partitions\n",
    "partition = la.find_partition(h, la.ModularityVertexPartition)\n",
    "print(len(partition))\n",
    "# optimiser = la.Optimiser()\n",
    "# # Optimize partitions\n",
    "# diff = optimiser.optimise_partition(partition, n_iterations=500)\n",
    "# print(len(partition))\n",
    "\n",
    "# print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leidenalg community assignments\n",
    "communities1 = partition.membership\n",
    "# Create a concatenated variable that combines groups 9 and above\n",
    "communities2 = [i if i < 9 else 9 for i in communities1]\n",
    "\n",
    "# # Create a mapping from igraph vertices to NetworkX nodes\n",
    "# Assuming nodes in G_trim are labeled from 0 to n-1\n",
    "mapping = {v.index: node for v, node in zip(h.vs, G_trim.nodes)}\n",
    "\n",
    "# Add community1 information to NetworkX graph\n",
    "for idx, community in enumerate(communities1):\n",
    "    nx_node = mapping[idx]\n",
    "    G_trim.nodes[nx_node]['community1'] = community\n",
    "\n",
    "\n",
    "# Add community2 information to NetworkX graph\n",
    "for idx, community in enumerate(communities2):\n",
    "    nx_node = mapping[idx]\n",
    "    G_trim.nodes[nx_node]['community2'] = community\n",
    "\n",
    "# Now G_trim has the community information\n",
    "# Example: Print node and its community\n",
    "# for node in G_trim.nodes(data=True):\n",
    "#     print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aef68",
   "metadata": {},
   "source": [
    "## 4.b. pygenstability (takes a long time and detects way too many communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pygenstability as pgs\n",
    "# import scipy.sparse as sp\n",
    "# import pygenstability as pgs\n",
    "# from pygenstability import plotting\n",
    "# from pygenstability.pygenstability import evaluate_NVI\n",
    "\n",
    "# A = nx.to_scipy_sparse_matrix(G_trim)\n",
    "# # run markov stability and identify optimal scales\n",
    "# results = pgs.run(\n",
    "#     A,\n",
    "#     min_scale=-1.25,\n",
    "#     max_scale=0.75,\n",
    "#     n_scale=50,\n",
    "#     n_tries=20,\n",
    "#     constructor=\"continuous_combinatorial\",\n",
    "#     n_workers=4\n",
    "# )\n",
    "# \n",
    "# pgs.plot_scan(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0709ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plotting.plot_scan(results, figure_name=None)\n",
    "# plotting.plot_optimal_partitions(G_trim, results, edge_color='0.5', edge_width=0.5, folder='optimal_partitions', ext='.pdf', show=False)\n",
    "# plot matrix\n",
    "# plt.figure()\n",
    "# plt.imshow(A, interpolation=\"nearest\")\n",
    "\n",
    "# print(f\"Number of nodes in G_trim: {len(G_trim.nodes)}\")\n",
    "# print(f\"Length of node_color array: {len(results['community_id'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13f5bb",
   "metadata": {},
   "source": [
    "## 4.c. Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab9ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write pajek to test on https://www.mapequation.org/infomap/#Input\n",
    "# # Create a copy of the graph without node attributes\n",
    "# G_trim_no_attrs = nx.Graph()\n",
    "\n",
    "# # Add nodes and edges to the new graph\n",
    "# G_trim_no_attrs.add_nodes_from(G_trim.nodes())\n",
    "# G_trim_no_attrs.add_edges_from(G_trim.edges())\n",
    "\n",
    "# # Write the new graph to a Pajek file\n",
    "# nx.write_pajek(G_trim_no_attrs, \"test_pajek_out.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infomap import Infomap\n",
    "\n",
    "# Create a mapping from original node labels to integers\n",
    "mapping = {node: i for i, node in enumerate(G_trim.nodes())}\n",
    "reverse_mapping = {i: node for node, i in mapping.items()}\n",
    "\n",
    "# Relabel the graph using the integer mapping\n",
    "G_trim_int = nx.relabel_nodes(G_trim, mapping)\n",
    "\n",
    "# Initialize 2 level Infomap\n",
    "infomap = Infomap(\"--two-level --directed --flow-model directed -v --ftree\")\n",
    "\n",
    "# Initialize multi-level Infomap\n",
    "# infomap = Infomap(\"--directed --flow-model directed -v \")\n",
    "\n",
    "# Add graph to Infomap\n",
    "infomap.add_networkx_graph(G_trim)\n",
    "\n",
    "# Run Infomap algorithm\n",
    "infomap.run()\n",
    "\n",
    "# Extract top-level map\n",
    "# network_flow = infomap.flow_network()\n",
    "# top_modules = infomap.modules()\n",
    "\n",
    "# # Create a new graph for the top-level map\n",
    "# G_top_level = nx.DiGraph()\n",
    "\n",
    "# # Add nodes representing communities\n",
    "# for module in top_modules:\n",
    "#     G_top_level.add_node(module)\n",
    "\n",
    "# # Add weighted edges based on flow between communities\n",
    "# for link in network_flow:\n",
    "#     source_module = link[0]\n",
    "#     target_module = link[1]\n",
    "#     flow = link[2]\n",
    "#     if source_module != target_module:  # Exclude self-loops if desired\n",
    "#         if G_top_level.has_edge(source_module, target_module):\n",
    "#             G_top_level[source_module][target_module]['weight'] += flow\n",
    "#         else:\n",
    "#             G_top_level.add_edge(source_module, target_module, weight=flow)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # infomap.write_flow_tree(\"test\")\n",
    "# # temp = nx.read_edgelist(\"test.csv\")\n",
    "# # len(temp.nodes())\n",
    "# # print(infomap.flow_links)\n",
    "# infomap_df = infomap.get_dataframe()\n",
    "# infomap.get_effective_num_modules()\n",
    "\n",
    "# # Get all links for individual nodes\n",
    "# # for link in infomap.get_links(data = \"weight\"):\n",
    "# #     print(link)\n",
    "# # for link in infomap.get_links(data = \"flow\"):\n",
    "# #     print(link)\n",
    "\n",
    "\n",
    "# # Get modules at depth of 1 (This is the same information from infomap_df col 1 and 4)\n",
    "# # infomap.get_modules(depth_level=1)\n",
    "\n",
    "# # for nodes in infomap.get_nodes(depth_level=1):\n",
    "# #     print(nodes) \n",
    "\n",
    "# print(infomap.get_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new graph for the top-level map\n",
    "# G_top_level = nx.DiGraph()\n",
    "\n",
    "# # Add nodes representing communities\n",
    "# for node in G_trim_int.nodes():\n",
    "#     community_id = infomap.get_modules()[node]\n",
    "#     G_top_level.add_node(community_id)\n",
    "\n",
    "# # Create a dictionary to store flow between communities\n",
    "# flow_between_communities = {}\n",
    "\n",
    "# # Add weighted edges based on flow between communities\n",
    "# for node, module in infomap.get_modules().items():\n",
    "#     original_node = reverse_mapping[node]\n",
    "#     for neighbor in G_trim.neighbors(original_node):\n",
    "#         neighbor_module = infomap.get_modules()[mapping[neighbor]]\n",
    "#         if module != neighbor_module:\n",
    "#             if (module, neighbor_module) not in flow_between_communities:\n",
    "#                 flow_between_communities[(module, neighbor_module)] = 0\n",
    "#             flow_between_communities[(module, neighbor_module)] += 1  # You can customize this to use actual flow values\n",
    "\n",
    "# # Add the weighted edges to the top-level graph\n",
    "# for (source_module, target_module), flow in flow_between_communities.items():\n",
    "#     G_top_level.add_edge(source_module, target_module, weight=flow)\n",
    "\n",
    "# # Optionally, print the new top-level graph to verify\n",
    "# for u, v, data in G_top_level.edges(data=True):\n",
    "#     print(f\"Edge from {u} to {v} with weight {data['weight']}\")\n",
    "\n",
    "# # Visualize the top-level graph\n",
    "# # pos = nx.spring_layout(G_top_level)\n",
    "# # edge_weights = [d['weight'] for (u, v, d) in G_top_level.edges(data=True)]\n",
    "# # nx.draw(G_top_level, pos, with_labels=True, node_size=500, node_color='lightblue', font_weight='bold', edge_color=edge_weights, edge_cmap=plt.cm.Blues, width=edge_weights)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d13895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Visualize the communities\n",
    "\n",
    "# # Assign a color to each community\n",
    "# colors = [infomap.get_modules()[mapping[node]] for node in G_trim.nodes()]\n",
    "\n",
    "# # Draw the graph\n",
    "# pos = nx.spring_layout(G_trim)\n",
    "# nx.draw_networkx_nodes(G_trim, pos, node_color=colors, cmap=plt.cm.tab20, node_size=\".\")\n",
    "# nx.draw_networkx_edges(G_trim, pos, alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIDNT WORK BECAUSE THERE IS NO VARIABILITY IN THE CAPACITY\n",
    "# from networkx.algorithms.flow import maximum_flow\n",
    "\n",
    "# # Assume G is your directed graph (DiGraph)\n",
    "# source_node = \"PFNA\"\n",
    "# target_node = \"C0011881\"\n",
    "\n",
    "# # Set a default capacity for all edges if not already present\n",
    "# for u, v in G.edges():\n",
    "#     if 'capacity' not in G[u][v]:\n",
    "#         G[u][v]['capacity'] = 10  # Set to 1 or any reasonable default value\n",
    "\n",
    "# # Calculate the original maximum flow\n",
    "# original_flow_value, original_flow_dict = maximum_flow(G, source_node, target_node)\n",
    "# print(f\"Original maximum flow from {source_node} to {target_node}: {original_flow_value}\")\n",
    "\n",
    "# # Dictionary to store the impact of each node on the maximum flow\n",
    "# node_impact = {}\n",
    "\n",
    "# # Perform leave-one-out analysis\n",
    "# for node in G.nodes:\n",
    "#     if node in [source_node, target_node]:\n",
    "#         continue  # Skip source and target nodes\n",
    "\n",
    "#     # Create a copy of the graph and remove the current node\n",
    "#     G_temp = G.copy()\n",
    "#     G_temp.remove_node(node)\n",
    "\n",
    "#     # Recalculate the maximum flow\n",
    "#     flow_value, flow_dict = maximum_flow(G_temp, source_node, target_node)\n",
    "\n",
    "#     # Calculate the difference in maximum flow\n",
    "#     flow_difference = original_flow_value - flow_value\n",
    "#     node_impact[node] = flow_difference\n",
    "\n",
    "# # Identify the nodes with the most significant impact on flow\n",
    "# most_impactful_nodes = sorted(node_impact.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Display the nodes and their impact\n",
    "# print(\"Node impact on maximum flow (sorted):\")\n",
    "# for node, impact in most_impactful_nodes:\n",
    "#     print(f\"Node: {node}, Impact: {impact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2197b",
   "metadata": {},
   "source": [
    "# 5. Node2vec analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780241a0",
   "metadata": {},
   "source": [
    "## 5.a. Node2vec (undirected graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~1 minute\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Generate Node2Vec model\n",
    "node2vec = Node2Vec(G_trim, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Extract embeddings for each node\n",
    "embeddings = model.wv\n",
    "node_ids = list(G_trim.nodes)\n",
    "node_embeddings = [embeddings[node] for node in node_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4f57d",
   "metadata": {},
   "source": [
    "## 5.b. TSNE/UMAP on node2vec results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run time ~ 10 seconds\n",
    "# Perform t-SNE to reduce dimensionality to 2D for visualization\n",
    "node_embeddings = np.array(node_embeddings)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_embeddings_2d = tsne.fit_transform(node_embeddings)\n",
    "\n",
    "# Note- JG tried umap, but it didn't look good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de2923",
   "metadata": {},
   "source": [
    "## 5.c. Cluster TSNE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060dab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run time ~ 20 seconds\n",
    "from sklearn.cluster import KMeans, DBSCAN, HDBSCAN, AgglomerativeClustering, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Perform K-Means clustering on the original node embeddings\n",
    "n_clusters = 5  # Set the number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_clusters = kmeans.fit_predict(node_embeddings)\n",
    "\n",
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "dbscan_clusters = dbscan.fit_predict(node_embeddings)\n",
    "\n",
    "# HDBSCAN clustering\n",
    "hdbscan = HDBSCAN(min_cluster_size= 8)\n",
    "hdbscan_clusters = hdbscan.fit_predict(node_embeddings)\n",
    "\n",
    "# Agglomerative Clustering\n",
    "agglomerativeclustering = AgglomerativeClustering(n_clusters= 5)\n",
    "agglomerative_clusters = agglomerativeclustering.fit_predict(node_embeddings)\n",
    "\n",
    "# Birch Clustering\n",
    "birch = Birch(n_clusters= 4)\n",
    "birch_clusters = birch.fit_predict(node_embeddings)\n",
    "\n",
    "# Gaussian Mixture Model clustering\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm_clusters = gmm.fit_predict(node_embeddings)\n",
    "\n",
    "\n",
    "print('K-means: ' + str(Counter(kmeans_clusters)))\n",
    "print('dbscan: '  + str(Counter(dbscan_clusters)))\n",
    "print('hdbscan: ' + str(Counter(hdbscan_clusters)))\n",
    "print('agglomerativeclustering: ' + str(Counter(agglomerative_clusters)))\n",
    "print('Birch: ' + str(Counter(birch_clusters)))\n",
    "print('GaussianMixture: ' + str(Counter(gmm_clusters)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072cc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame linking node names with their 2D embeddings\n",
    "df_embeddings = pd.DataFrame(tsne_embeddings_2d, columns=['tsne_dim1', 'tsne_dim2'])\n",
    "# umap_embeddings = pd.DataFrame(umap_embeddings_2d, columns=['umap_dim1', 'umap_dim2'])\n",
    "# df_embeddings = pd.merge(umap_embeddings, tsne_embeddings,  left_index=True, right_index=True)\n",
    "df_embeddings['gene'] = node_ids\n",
    "\n",
    "# Reorder columns to have 'Node' first\n",
    "df_embeddings = df_embeddings[['gene', 'tsne_dim1', 'tsne_dim2']] # , 'umap_dim1', 'umap_dim2'\n",
    "\n",
    "# Merge with clusters \n",
    "df_embeddings['kmeans_clusters']        = kmeans_clusters\n",
    "df_embeddings['dbscan_clusters']        = dbscan_clusters\n",
    "df_embeddings['hdbscan_clusters']       = hdbscan_clusters\n",
    "df_embeddings['agglomerative_clusters'] = agglomerative_clusters\n",
    "df_embeddings['birch']                  = birch_clusters\n",
    "df_embeddings['gmm_clusters']           = gmm_clusters\n",
    "\n",
    "# Before merge: record the number of rows in each dataframe\n",
    "rows_before = {'df_embeddings': len(df_embeddings), 'gene_data': len(gene_data)}\n",
    "\n",
    "# Merging with gene_data metadata, by 'gene' and 'EntrezGeneSymbol' columns\n",
    "df_merged = pd.merge(df_embeddings, gene_data, how='left', left_on='gene', right_on='EntrezGeneSymbol')\n",
    "\n",
    "\n",
    "# After merge: record the number of rows in the merged dataframe\n",
    "rows_after = len(df_merged)\n",
    "\n",
    "# Print the merge details\n",
    "print(f\"Number of rows in df_embeddings before merge: {rows_before['df_embeddings']}\")\n",
    "print(f\"Number of rows in gene_data before merge: {rows_before['gene_data']}\")\n",
    "print(f\"Number of rows in df_merged after merge: {rows_after}\")\n",
    "print(f\"Number of rows matched: {rows_after}\")\n",
    "print(f\"Number of rows lost from df_embeddings: {rows_before['df_embeddings'] - rows_after}\")\n",
    "print(f\"Number of rows lost from original gene_data: {rows_before['gene_data'] - rows_after}\")\n",
    "# print(df_merged.columns)\n",
    "\n",
    "df_merged.to_csv(dir_res / 'network_graph_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd00edd",
   "metadata": {},
   "source": [
    "## 5.c. Plot TSNE Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign numeric codes to categories for coloring\n",
    "categories      = pd.Categorical(df_merged['sig_overall'])\n",
    "category_colors = pd.Categorical(df_merged['sig_overall']).codes\n",
    "\n",
    "# List of cluster columns to plot\n",
    "cluster_columns = [\n",
    "    'kmeans_clusters', \n",
    "    'dbscan_clusters', \n",
    "    'hdbscan_clusters', \n",
    "    'agglomerative_clusters', \n",
    "    'birch', \n",
    "    'gmm_clusters'\n",
    "]\n",
    "\n",
    "# Set up the figure and axes for the subplots\n",
    "fig, axes = plt.subplots(1, 6, figsize=(24, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Iterate over the cluster columns and axes to create scatter plots\n",
    "for ax, column in zip(axes, cluster_columns):\n",
    "    scatter = ax.scatter(df_merged['tsne_dim1'], df_merged['tsne_dim2'], c=df_merged[column], cmap='viridis', alpha=0.6, s=10)\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('t-SNE Dimension 1')\n",
    "    ax.set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Add a color bar to the right of the plots\n",
    "# cbar = fig.colorbar(scatter, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "# cbar.set_label('Cluster Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2812e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_columns = [\n",
    "    'kmeans_clusters', \n",
    "    'dbscan_clusters', \n",
    "    'hdbscan_clusters', \n",
    "    'agglomerative_clusters', \n",
    "    'birch', \n",
    "    'gmm_clusters'\n",
    "]\n",
    "\n",
    "# Cross tabs\n",
    "cross_table = pd.crosstab(df_merged['sig_overall'], df_merged['birch'])\n",
    "# Convert counts to percentages\n",
    "# cross_table_percent = cross_table.div(cross_table.sum(axis=1), axis=0) * 100\n",
    "\n",
    "cross_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52280e8",
   "metadata": {},
   "source": [
    "# 6. Save Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81caba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge G_trim and df_merged \n",
    "temp_df = df_merged.copy()\n",
    "G_fin = G_trim.copy()\n",
    "\n",
    "# Set the 'gene' column as the index for easy lookup\n",
    "df_merged_fin = temp_df.loc[:, 'tsne_dim1':'gmm_clusters']\n",
    "df_merged_fin['gene'] = temp_df['EntrezGeneSymbol']\n",
    "# df_merged_fin['gene'] = temp_df['gene']\n",
    "df_merged_fin.set_index('gene', inplace=True)\n",
    "\n",
    "# Iterate through the nodes in G_trim and add attributes from df_merged\n",
    "for node in G_fin.nodes():\n",
    "    if node in df_merged.index:\n",
    "        for attr in df_merged.columns:\n",
    "            G_trim.nodes[node][attr] = df_merged.loc[node, attr]\n",
    "\n",
    "# Example: Print node and its attributes\n",
    "# for node in G_fin.nodes(data=True):\n",
    "#     print(node)\n",
    "\n",
    "# # Save file\n",
    "# file_path = dir_res / 'ComptoxAI' / 'PFAS_prot_in_vitro_sig_fdr_trimmed_073024.graphml'\n",
    "file_path = dir_res / 'ComptoxAI' / 'PFAS_prot_in_vitro_sig_091224.graphml'\n",
    "\n",
    "# Write the graph to a GraphML file\n",
    "nx.write_graphml(G_fin, file_path)\n",
    "print(f\"Network saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign numeric codes to categories for coloring\n",
    "categories      = pd.Categorical(df_merged['sig_overall'])\n",
    "category_colors = pd.Categorical(df_merged['sig_overall']).codes\n",
    "\n",
    "# List of cluster columns to plot\n",
    "cluster_columns = [\n",
    "    'kmeans_clusters', \n",
    "    'dbscan_clusters', \n",
    "    'hdbscan_clusters', \n",
    "    'agglomerative_clusters', \n",
    "    'birch', \n",
    "    'gmm_clusters'\n",
    "]\n",
    "\n",
    "\n",
    "# Set up the figure and axes for the subplots\n",
    "fig, axes = plt.subplots(1, 6, figsize=(24, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Iterate over the cluster columns and axes to create scatter plots\n",
    "for ax, column in zip(axes, cluster_columns):\n",
    "    scatter = ax.scatter(df_merged['tsne_dim1'], df_merged['tsne_dim2'], c=df_merged[column], cmap='viridis', alpha=0.6, s=10)\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel('t-SNE Dimension 1')\n",
    "    ax.set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Add a color bar to the right of the plots\n",
    "# cbar = fig.colorbar(scatter, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "# cbar.set_label('Cluster Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021dc1a",
   "metadata": {},
   "source": [
    "# 6. GraphSAGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e05b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph needs to have the same attributes for this algorithm\n",
    "def get_all_attributes(G_trim):\n",
    "    node_attributes = set()\n",
    "    edge_attributes = set()\n",
    "\n",
    "    # Collect node attributes\n",
    "    for node, attr in G_trim.nodes(data=True):\n",
    "        node_attributes.update(attr.keys())\n",
    "\n",
    "    # Collect edge attributes\n",
    "    for u, v, attr in G_trim.edges(data=True):\n",
    "        edge_attributes.update(attr.keys())\n",
    "\n",
    "    return node_attributes, edge_attributes\n",
    "\n",
    "# Get unique attributes\n",
    "node_attrs, edge_attrs = get_all_attributes(G)\n",
    "print(f\"Unique node attributes: {node_attrs}\")\n",
    "print(f\"Unique edge attributes: {edge_attrs}\")\n",
    "\n",
    "# Desired attributes to keep\n",
    "desired_attributes = {'commonName', 'type', 'typeOfGene'}\n",
    "\n",
    "# Make copy of data\n",
    "G_reduced = G_trim.copy()\n",
    "\n",
    "# Function to filter node attributes\n",
    "def filter_node_attributes(G_reduced, desired_attributes):\n",
    "    for node in G_reduced.nodes:\n",
    "        current_attributes = G_reduced.nodes[node]\n",
    "        filtered_attributes = {k: v for k, v in current_attributes.items() if k in desired_attributes}\n",
    "        G_reduced.nodes[node].clear()\n",
    "        G_reduced.nodes[node].update(filtered_attributes)\n",
    "\n",
    "# # Apply the function to filter node attributes\n",
    "filter_node_attributes(G_reduced, desired_attributes)\n",
    "\n",
    "# Get unique attributes\n",
    "node_attrs, edge_attrs = get_all_attributes(G_reduced)\n",
    "print(f\"Filtered unique node attributes: {node_attrs}\")\n",
    "print(f\"Filtered unique edge attributes: {edge_attrs}\")\n",
    "\n",
    "# Set values for nodes\n",
    "G_reduced.nodes['C0011881']['typeOfGene'] = 'disease' \n",
    "G_reduced.nodes['PFNA']['typeOfGene'] = 'Chemical'  \n",
    "\n",
    "# Check which nodes are missing the \"typeOfGene\" attribute\n",
    "missing_typeOfGene_nodes = [node for node, attr in G_reduced.nodes(data=True) if 'typeOfGene' not in attr]\n",
    "print(f\"Number of missing typeOfGene: {len(missing_typeOfGene_nodes)}\")\n",
    "\n",
    "# Check which nodes are missing the \"typeOfGene\" attribute\n",
    "missing_typeOfGene_nodes = [node for node, attr in G_reduced.nodes(data=True) if 'type' not in attr]\n",
    "print(f\"Number of missing type: {len(missing_typeOfGene_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Example graph data (this should be replaced with your actual graph data)\n",
    "# Assume G is a networkx directed graph with node features and labels\n",
    "\n",
    "# Convert NetworkX graph to PyTorch Geometric Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Create PyTorch Geometric data from NetworkX graph\n",
    "data = from_networkx(G_reduced)\n",
    "\n",
    "# Ensure that data.x (node features) and data.y (labels) are present\n",
    "# Here, we assume each node has a feature vector and a label\n",
    "\n",
    "# Example of adding random node features and labels for demonstration\n",
    "import numpy as np\n",
    "num_nodes = data.num_nodes\n",
    "num_features = 16  # Example feature size\n",
    "num_classes = 3    # Example number of classes\n",
    "\n",
    "data.x = torch.tensor(np.random.randn(num_nodes, num_features), dtype=torch.float)\n",
    "data.y = torch.tensor(np.random.randint(0, num_classes, num_nodes), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5ff8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f3f42a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.a. (OLD) Identify all nodes linking PFNA, genes, and the outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f248f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cypher query\n",
    "# Start and end strings\n",
    "start_string = \"MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (end:Gene) WHERE end.geneSymbol IN \"\n",
    "\n",
    "middle_string = \"CALL apoc.path.spanningTree(d, {relationshipFilter: '<GENEASSOCIATESWITHDISEASE|GENEINTERACTSWITHGENE', minLevel: 1, maxLevel: 5, endNodes: end}) YIELD path WITH nodes(path) as n RETURN n UNION MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (end:Gene) WHERE end.geneSymbol IN\"\n",
    "\n",
    "end_string = \"CALL apoc.path.spanningTree(c, {relationshipFilter: 'CHEMICALINCREASESEXPRESSION>|CHEMICALDECREASESEXPRESSION>|GENEINTERACTSWITHGENE>', minLevel: 1, maxLevel: 7, endNodes: end}) YIELD path WITH nodes(path) as n RETURN n;\"\n",
    "\n",
    "# Combine the start and end strings with the unique_gene_symbols_list\n",
    "first_query = start_string + Entrez_Genes + middle_string + Entrez_Genes + end_string\n",
    "\n",
    "# Run first query to identify all nodes\n",
    "nodes = db.run_cypher(first_query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlist everything and get a list of separate dictionaries\n",
    "flattened_list = [inner_dict for outer_dict in nodes for inner_dict in outer_dict['n']]\n",
    "\n",
    "# Remove duplicates by converting to a set and back to a list\n",
    "unique_list = list({tuple(item.items()) for item in flattened_list})\n",
    "\n",
    "# Convert the list of tuples back to dictionaries\n",
    "unique_list = [dict(item) for item in unique_list]\n",
    "\n",
    "print('The number of unique nodes, including the PFAS and disease node, is ' +  str(len(unique_list)))\n",
    "# Look at first few elements of list\n",
    "# pprint(unique_list[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract geneSymbol\n",
    "# Create a set to store unique geneSymbol values\n",
    "unique_gene_symbols = set()\n",
    "\n",
    "# Iterate through the list and collect unique geneSymbol values\n",
    "for dictionary in flattened_list:\n",
    "    if 'geneSymbol' in dictionary:\n",
    "        unique_gene_symbols.add(dictionary['geneSymbol'])\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "unique_gene_symbols_list = list(unique_gene_symbols)\n",
    "\n",
    "# Now, 'unique_gene_symbols_list' contains all unique geneSymbol values\n",
    "\n",
    "print('The number of unique genes is ' +  str(len(unique_gene_symbols_list)) + \", and \" + str(len(set(unique_gene_symbols_list) & set(Entrez_Gene_Symbols))) + \" of these genes are from the original data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f382b139",
   "metadata": {},
   "source": [
    "## 1.b. (OLD) With all nodes, identify all of the relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c99959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cypher query\n",
    "# Start and end strings\n",
    "start_string = \"MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (node1:Gene) WHERE node1.geneSymbol IN \"\n",
    "\n",
    "end_string = \"WITH collect(id(node1))+collect(c)+collect(d) as nodes CALL apoc.algo.cover(nodes) YIELD rel RETURN  startNode(rel), rel, endNode(rel);\"\n",
    "\n",
    "# Combine the start and end strings with the unique_gene_symbols_list\n",
    "query_string = start_string + \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in unique_gene_symbols_list]) + \"]\" + end_string\n",
    "\n",
    "# Run Cypher Query\n",
    "data = db.run_cypher(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(type(data))\n",
    "print(type(data[1]))\n",
    "#pprint(data[9])\n",
    " \n",
    "# Find elements with 'CHEMICALINCREASESEXPRESSION' in the 'rel' part\n",
    "filtered_elements = [element for element in data if any('CHEMICALINCREASESEXPRESSION' in item for item in element['rel'])]\n",
    "# Print the filtered elements\n",
    "#for idx, element in enumerate(filtered_elements):\n",
    "#    print(f\"Element {idx + 1}:\")\n",
    "#    print(element)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
