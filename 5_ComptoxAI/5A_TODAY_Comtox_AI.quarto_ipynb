{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Untitled\"\n",
        "format: html\n",
        "---"
      ],
      "id": "1a7b2e64"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python version\n",
        "! python --version"
      ],
      "id": "python-version",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Set up project and read in data "
      ],
      "id": "7371cc15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Read Data\n",
        "# Set up \n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# Get the current directory\n",
        "current_directory = Path.cwd()\n",
        "\n",
        "# Get folder names\n",
        "dir_dat = current_directory.parent.parent / \"0_data\" / \"clean_data\"\n",
        "dir_res = current_directory.parent.parent / \"2_results\"\n",
        "\n",
        "\n",
        "# Read in proteomics metadata\n",
        "# gene_data = pd.read_csv(dir_res / 'Combined_proteomics_scRNAseq_sig_results_073024.csv')\n",
        "gene_data = pd.read_csv(dir_res / 'Combined_proteomics_scRNAseq_sig_fdr_results_090924.csv')\n",
        "\n",
        "# Get names of genes in a list\n",
        "gene_list = gene_data['EntrezGeneSymbol'].tolist()\n",
        "\n",
        "# Combine Genes into str\n",
        "genes = \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in gene_list]) + \"]\"\n",
        "\n",
        "overlapping_gene_list = [\"TMEM87B\", \"THOC1\", \"SPRED1\", \"MFN1\"]\n",
        "#print(split_genes)\n",
        "#print(split_genes)\n",
        "# type(gene_data)\n",
        "#print(type(prot_metadata.EntrezGeneSymbol))"
      ],
      "id": "Read-Data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List of genes in single cell"
      ],
      "id": "d91f8971"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Gene Lists\n",
        "genes_both_human_invitro = gene_data[gene_data['sig_overall_simplified'] == 'Both']['EntrezGeneSymbol'].tolist()\n",
        "genes_human_only = gene_data[gene_data['sig_overall_simplified'] == 'TODAY']['EntrezGeneSymbol'].tolist()\n",
        "genes_in_vitro_only = gene_data[gene_data['sig_overall_simplified'] == 'scRNAseq']['EntrezGeneSymbol'].tolist()"
      ],
      "id": "Gene-Lists",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Query ComptoxAI database using Neo4j. \n",
        "Note: to get this to run, you have to have an Neo4j instance of comptoxai."
      ],
      "id": "b1af926a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: ComptoxAI query\n",
        "# Connect to neo4j database\n",
        "import comptox_ai\n",
        "from comptox_ai.db.graph_db import GraphDB\n",
        "db = GraphDB(username=\"cytoscape\", password= \"12345\", hostname=\"localhost:7687\")\n",
        "#db = GraphDB(username=\"neo4j_user\", password=\"12345\", hostname=\"localhost:7687\")\n",
        "\n",
        "# Create cypher query\n",
        "# Start and end strings\n",
        "start_string = \"MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (node1:Gene) WHERE node1.geneSymbol IN \"\n",
        "\n",
        "end_string = \"WITH collect(id(node1))+collect(c)+collect(d) as nodes CALL apoc.algo.cover(nodes) YIELD rel RETURN  startNode(rel), rel, endNode(rel);\"\n",
        "\n",
        "# Combine the start and end strings with the unique_gene_symbols_list\n",
        "query_string = start_string + \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in gene_list]) + \"] \" + end_string\n",
        "\n",
        "# Run Cypher Query\n",
        "data = db.run_cypher(query_string)\n",
        "# print(query_string)"
      ],
      "id": "ComptoxAI-query",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Create Graph\n",
        "\n",
        "## 2.a. Create Base Graph from ComptoxAI query\n"
      ],
      "id": "c6834613"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Create network diagram\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a new graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# List mediator proteins\n",
        "\n",
        "# Function to compute the combined 'type' attribute\n",
        "def compute_type(node):\n",
        "    if node.get('commonName') == 'PFNA':\n",
        "        return 'PFAS'\n",
        "    if node.get('commonName') == 'Diabetic Nephropathy':\n",
        "        return 'Disease'\n",
        "    if node.get('geneSymbol') in genes_both_human_invitro:\n",
        "        return 'Gene: Human and in-vitro'\n",
        "    if node.get('geneSymbol') in genes_human_only:\n",
        "        return 'Gene: Human only'\n",
        "    if node.get('geneSymbol') in genes_in_vitro_only:\n",
        "        return 'Gene: In-vitro only'\n",
        "    else:\n",
        "        return 'Gene: from ComptoxAI'\n",
        "\n",
        "    \n",
        "# Add nodes and edges with combined 'type' attribute\n",
        "for entry in data:\n",
        "    start_node = entry['startNode(rel)']\n",
        "    end_node = entry['endNode(rel)']\n",
        "    rel_type = entry['rel'][1]  # Relationship type is the second item in the tuple\n",
        "    \n",
        "    # Set combined 'type' attribute\n",
        "    start_node['type'] = compute_type(start_node)\n",
        "    end_node['type'] = compute_type(end_node)\n",
        "\n",
        "    # Node identifiers\n",
        "    start_node_id = start_node.get('geneSymbol')  or start_node.get('commonName')\n",
        "    end_node_id = end_node.get('geneSymbol') or end_node.get('xrefUmlsCUI') \n",
        "\n",
        "    # Add nodes with combined 'type' attribute\n",
        "    G.add_node(start_node_id, **start_node)\n",
        "    G.add_node(end_node_id, **end_node)\n",
        "\n",
        "    # Add edge\n",
        "    G.add_edge(start_node_id, end_node_id, relationship=rel_type, weight = 1)\n",
        "\n",
        "del data "
      ],
      "id": "Create-network-diagram",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.b Create figure"
      ],
      "id": "b98f2962"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Color mapping\n",
        "color_map = {\n",
        "    'PFAS': 'magenta',\n",
        "    'Disease': 'red',\n",
        "    'Gene: Human and in-vitro': 'green',\n",
        "    'Gene: Human only': 'blue', \n",
        "    'Gene: In-vitro only': 'grey'\n",
        "}\n",
        "\n",
        "# Compute node colors based on 'type' attribute\n",
        "node_colors = [color_map[G.nodes[node]['type']] for node in G]\n",
        "\n",
        "print(Counter(node_colors))\n",
        "print(len(G))\n",
        "#print(len(node_colors))\n",
        "\n",
        "# Draw the graph\n",
        "#plt.figure(figsize=(12, 8))  # Set the figure size\n",
        "#pos = nx.spring_layout(G)  # Layout for the nodes\n",
        "#nx.draw_networkx(G, pos, with_labels=True, node_color=node_colors, node_size=700, edge_color='k', linewidths=1, font_size=10, arrows=True)\n",
        "#nx.draw_networkx_edge_labels(G, pos, edge_labels=nx.get_edge_attributes(G, 'relationship'))\n",
        "\n",
        "#plt.show()"
      ],
      "id": "3e56bec2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Check graph for issues"
      ],
      "id": "7108ded0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Check self loops\n",
        "# Check graph after adding metadata\n",
        "isolated_nodes = list(nx.isolates(G))\n",
        "print(nx.number_of_isolates(G))\n",
        "print(len(G.nodes))\n",
        "\n",
        "# Examine self loop edges:\n",
        "self_loop_edges = list(nx.selfloop_edges(G))\n",
        "# Extract nodes that have self-loops\n",
        "self_loop_nodes = set(node for node, _ in self_loop_edges)\n",
        "# Identify nodes that only have self-loops and no other connections\n",
        "isolated_self_loop_nodes = [\n",
        "    node for node in self_loop_nodes\n",
        "    if G.degree(node) == 1  # Total degree (in-degree + out-degree) is 1, indicating only a self-loop\n",
        "]\n",
        "print(f\"Nodes only connected to themselves: {isolated_self_loop_nodes}\")"
      ],
      "id": "Check-self-loops",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.a Identify disconnected graphs"
      ],
      "id": "9ea80a13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: identify disconnected graphs\n",
        "# Generate weakly connected components\n",
        "weakly_connected_components = nx.weakly_connected_components(G)\n",
        "\n",
        "# Get the size of each component (number of nodes)\n",
        "component_sizes = [len(component) for component in weakly_connected_components]\n",
        "# print(Counter(component_sizes))\n",
        "\n",
        "# Identify genes not in the large componenet -----------\n",
        "\n",
        "# Find all weakly connected components\n",
        "weakly_connected_components = list(nx.weakly_connected_components(G))\n",
        "\n",
        "# Find the largest weakly connected component (by number of nodes)\n",
        "largest_component = max(weakly_connected_components, key=len)\n",
        "\n",
        "# Find all nodes in the graph\n",
        "all_nodes = set(G.nodes())\n",
        "\n",
        "# Find nodes not in the largest component\n",
        "nodes_not_in_largest = all_nodes - largest_component\n",
        "\n",
        "# Prepare the data for the table\n",
        "data = []\n",
        "for node in nodes_not_in_largest:\n",
        "    node_type = G.nodes[node].get('type', 'Unknown')  # Use 'Unknown' if no type is provided\n",
        "    data.append((node, node_type))\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=['Node', 'Type'])\n",
        "\n",
        "# Sort the DataFrame by the 'Type' column\n",
        "df_sorted = df.sort_values(by='Type')\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "#print(df_sorted)\n",
        "\n",
        "# Remove these nodes from the graph\n",
        "G_trim = G.copy()\n",
        "G_trim.remove_nodes_from(nodes_not_in_largest)\n",
        "\n",
        "print(len(G.nodes))\n",
        "print(len(G_trim.nodes))\n",
        "print(len(nodes_not_in_largest))\n",
        "print(len(G_trim.nodes) == len(G.nodes)-len(nodes_not_in_largest))\n",
        "print(nodes_not_in_largest)"
      ],
      "id": "identify-disconnected-graphs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Analyze communities and paths between PFNA and DKD\n",
        "\n",
        "## 4.a. leidenalg"
      ],
      "id": "639c8a8a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: compute leidenalg Communities\n",
        "import igraph as ig\n",
        "import leidenalg as la\n",
        "import cairocffi as cairo\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "source_node = \"PFNA\"\n",
        "target_node = \"C0011881\"\n",
        "\n",
        "# convert to igraph\n",
        "h = ig.Graph.from_networkx(G_trim)\n",
        "# Create a new attribute 'name' from '_nx_name'\n",
        "h.vs['name'] = h.vs['_nx_name']\n",
        "\n",
        "# Create a dictionary to map gene symbols to sig_overall values\n",
        "gene_to_sig = dict(zip(gene_data['EntrezGeneSymbol'], gene_data['sig_overall']))\n",
        "\n",
        "# Initialize the cluster assignments for nodes based on 'sig_overall'\n",
        "initial_clusters = []\n",
        "for vertex in h.vs:\n",
        "    gene_symbol = vertex['name']\n",
        "    if gene_symbol in gene_to_sig:\n",
        "        initial_clusters.append(gene_to_sig[gene_symbol])\n",
        "    elif gene_symbol == source_node:\n",
        "        initial_clusters.append(source_node)\n",
        "    elif gene_symbol == target_node:\n",
        "        initial_clusters.append(target_node)    \n",
        "    else:\n",
        "        initial_clusters.append(\"error\")  # Assign -1 if no match is found\n",
        "\n",
        "# Convert the list of strings to a numeric list using pandas' factorize function\n",
        "import pandas as pd\n",
        "initial_clusters_num, _ = pd.factorize(pd.Series(initial_clusters))\n",
        "initial_clusters = initial_clusters_num.tolist()\n",
        "\n",
        "# Set the initial cluster assignment as a vertex attribute\n",
        "h.vs['initial_cluster'] = initial_clusters\n",
        "Counter(initial_clusters)\n",
        "\n",
        "# Identify cluster partitions\n",
        "partition = la.find_partition(graph=h, partition_type=la.ModularityVertexPartition, initial_membership=initial_clusters, n_iterations=-1, seed = 3787)\n",
        "print(len(partition))\n",
        "# Optimize partitions\n",
        "optimiser = la.Optimiser()\n",
        "diff = optimiser.optimise_partition(partition, n_iterations=1000)\n",
        "print(len(partition))\n",
        "print(diff)\n",
        "\n",
        "#| label: Get leidenalg community assignments\n",
        "leidenalg_community = partition.membership\n",
        "# Create a concatenated variable that combines groups 9 and above\n",
        "leidenalg_community_small = [i if i < 9 else 9 for i in communities1]\n",
        "leidenalg_community_min = [i if i < 3 else 3 for i in communities1]\n",
        "\n",
        "source_vertex = h.vs.find(name=source_node)\n",
        "target_vertex = h.vs.find(name=target_node)\n",
        "source_cluster = partition.membership[source_vertex.index]\n",
        "target_cluster = partition.membership[target_vertex.index]\n",
        "source_cluster_size = Counter(leidenalg_community)[source_cluster]\n",
        "target_cluster_size = Counter(leidenalg_community)[target_cluster]\n",
        "print(f'PFNA Cluster: {source_cluster}, cluster size: {source_cluster_size}')\n",
        "print(f'DKD Cluster: {target_cluster}, cluster size:  {target_cluster_size}')\n",
        "\n",
        "#| label: Get leidenalg community assignments\n",
        "leidenalg_community = partition.membership\n",
        "# Create a concatenated variable that combines groups 3 and above\n",
        "leidenalg_community_min = [i if i < 3 else 3 for i in leidenalg_community]"
      ],
      "id": "compute-leidenalg-Communities",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # Create a mapping from igraph vertices to NetworkX nodes\n",
        "# Assuming nodes in G_trim are labeled from 0 to n-1\n",
        "# mapping = {v.index: node for v, node in zip(h.vs, G_trim.nodes)}\n",
        "\n",
        "# # # Add leidenalg_community information to NetworkX graph\n",
        "# # for idx, community in enumerate(leidenalg_community):\n",
        "# #     nx_node = mapping[idx]\n",
        "# #     G_trim.nodes[nx_node]['leidenalg_community'] = leidenalg_community\n",
        "\n",
        "# # # Add leidenalg_community_red information to NetworkX graph\n",
        "# # for idx, community in enumerate(leidenalg_community_min):\n",
        "# #     nx_node = mapping[idx]\n",
        "# #     G_trim.nodes[nx_node]['leidenalg_community_min'] = leidenalg_community_min\n",
        "\n",
        "# # # Now G_trim has the community information\n",
        "# # # Example: Print node and its community\n",
        "# # # for node in G_trim.nodes(data=True):\n",
        "# # #     print(node)"
      ],
      "id": "0beed316",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.b. Find all simple paths from source_node to target_node"
      ],
      "id": "0bca6e61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find all simple paths from source_node to target_node\n",
        "all_paths = list(nx.shortest_path(G_trim, source=source_node, target=target_node))\n",
        "print(all_paths)"
      ],
      "id": "10743184",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.c. pygenstability (not run)\n",
        "takes a long time and detects way too many communities"
      ],
      "id": "ae38fcda"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: pygenstability (NOT RUN)\n",
        "# import pygenstability as pgs\n",
        "# import scipy.sparse as sp\n",
        "# import pygenstability as pgs\n",
        "# from pygenstability import plotting\n",
        "# from pygenstability.pygenstability import evaluate_NVI\n",
        "\n",
        "# A = nx.to_scipy_sparse_matrix(G_trim)\n",
        "# # run markov stability and identify optimal scales\n",
        "# results = pgs.run(\n",
        "#     A,\n",
        "#     min_scale=-1.25,\n",
        "#     max_scale=0.75,\n",
        "#     n_scale=50,\n",
        "#     n_tries=20,\n",
        "#     constructor=\"continuous_combinatorial\",\n",
        "#     n_workers=4\n",
        "# )\n",
        "# \n",
        "# pgs.plot_scan(results)\n",
        "# \n",
        "# plotting.plot_scan(results, figure_name=None)\n",
        "# plotting.plot_optimal_partitions(G_trim, results, edge_color='0.5', edge_width=0.5, folder='optimal_partitions', ext='.pdf', show=False)\n",
        "# plot matrix\n",
        "# plt.figure()\n",
        "# plt.imshow(A, interpolation=\"nearest\")\n",
        "# \n",
        "# print(f\"Number of nodes in G_trim: {len(G_trim.nodes)}\")\n",
        "# print(f\"Length of node_color array: {len(results['community_id'])}\")"
      ],
      "id": "pygenstability-NOT-RUN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.d. Infomap (not used)"
      ],
      "id": "8c32a9fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Infomap\n",
        "# # # Write pajek to test on https://www.mapequation.org/infomap/#Input\n",
        "# # # Create a copy of the graph without node attributes\n",
        "# # G_trim_no_attrs = nx.Graph()\n",
        "\n",
        "# # # Add nodes and edges to the new graph\n",
        "# # G_trim_no_attrs.add_nodes_from(G_trim.nodes())\n",
        "# # G_trim_no_attrs.add_edges_from(G_trim.edges())\n",
        "\n",
        "# # # Write the new graph to a Pajek file\n",
        "# # nx.write_pajek(G_trim_no_attrs, \"test_pajek_out.net\")\n",
        "# from infomap import Infomap\n",
        "\n",
        "# # Create a mapping from original node labels to integers\n",
        "# mapping = {node: i for i, node in enumerate(G_trim.nodes())}\n",
        "# reverse_mapping = {i: node for node, i in mapping.items()}\n",
        "\n",
        "# # Relabel the graph using the integer mapping\n",
        "# G_trim_int = nx.relabel_nodes(G_trim, mapping)\n",
        "\n",
        "# # Initialize 2 level Infomap\n",
        "# infomap = Infomap(\"--two-level --directed --flow-model directed -v --ftree\")\n",
        "\n",
        "# # Initialize multi-level Infomap\n",
        "# # infomap = Infomap(\"--directed --flow-model directed -v \")\n",
        "\n",
        "# # Add graph to Infomap\n",
        "# infomap.add_networkx_graph(G_trim)\n",
        "\n",
        "# # Run Infomap algorithm\n",
        "# infomap.run()\n",
        "\n",
        "# Extract top-level map\n",
        "# network_flow = infomap.flow_network()\n",
        "# top_modules = infomap.modules()\n",
        "\n",
        "# # Create a new graph for the top-level map\n",
        "# G_top_level = nx.DiGraph()\n",
        "\n",
        "# # Add nodes representing communities\n",
        "# for module in top_modules:\n",
        "#     G_top_level.add_node(module)\n",
        "\n",
        "# # Add weighted edges based on flow between communities\n",
        "# for link in network_flow:\n",
        "#     source_module = link[0]\n",
        "#     target_module = link[1]\n",
        "#     flow = link[2]\n",
        "#     if source_module != target_module:  # Exclude self-loops if desired\n",
        "#         if G_top_level.has_edge(source_module, target_module):\n",
        "#             G_top_level[source_module][target_module]['weight'] += flow\n",
        "#         else:\n",
        "#             G_top_level.add_edge(source_module, target_module, weight=flow)\n",
        "\n",
        "# # infomap.write_flow_tree(\"test\")\n",
        "# # temp = nx.read_edgelist(\"test.csv\")\n",
        "# # len(temp.nodes())\n",
        "# # print(infomap.flow_links)\n",
        "# infomap_df = infomap.get_dataframe()\n",
        "# infomap.get_effective_num_modules()\n",
        "\n",
        "# # Get all links for individual nodes\n",
        "# # for link in infomap.get_links(data = \"weight\"):\n",
        "# #     print(link)\n",
        "# # for link in infomap.get_links(data = \"flow\"):\n",
        "# #     print(link)\n",
        "\n",
        "\n",
        "# # Get modules at depth of 1 (This is the same information from infomap_df col 1 and 4)\n",
        "# # infomap.get_modules(depth_level=1)\n",
        "\n",
        "# # for nodes in infomap.get_nodes(depth_level=1):\n",
        "# #     print(nodes) \n",
        "\n",
        "# print(infomap.get_nodes)\n",
        "\n",
        "# # Create a new graph for the top-level map\n",
        "# G_top_level = nx.DiGraph()\n",
        "\n",
        "# # Add nodes representing communities\n",
        "# for node in G_trim_int.nodes():\n",
        "#     community_id = infomap.get_modules()[node]\n",
        "#     G_top_level.add_node(community_id)\n",
        "\n",
        "# # Create a dictionary to store flow between communities\n",
        "# flow_between_communities = {}\n",
        "\n",
        "# # Add weighted edges based on flow between communities\n",
        "# for node, module in infomap.get_modules().items():\n",
        "#     original_node = reverse_mapping[node]\n",
        "#     for neighbor in G_trim.neighbors(original_node):\n",
        "#         neighbor_module = infomap.get_modules()[mapping[neighbor]]\n",
        "#         if module != neighbor_module:\n",
        "#             if (module, neighbor_module) not in flow_between_communities:\n",
        "#                 flow_between_communities[(module, neighbor_module)] = 0\n",
        "#             flow_between_communities[(module, neighbor_module)] += 1  # You can customize this to use actual flow values\n",
        "\n",
        "# # Add the weighted edges to the top-level graph\n",
        "# for (source_module, target_module), flow in flow_between_communities.items():\n",
        "#     G_top_level.add_edge(source_module, target_module, weight=flow)\n",
        "\n",
        "# # Optionally, print the new top-level graph to verify\n",
        "# for u, v, data in G_top_level.edges(data=True):\n",
        "#     print(f\"Edge from {u} to {v} with weight {data['weight']}\")\n",
        "\n",
        "# # Visualize the top-level graph\n",
        "# # pos = nx.spring_layout(G_top_level)\n",
        "# # edge_weights = [d['weight'] for (u, v, d) in G_top_level.edges(data=True)]\n",
        "# # nx.draw(G_top_level, pos, with_labels=True, node_size=500, node_color='lightblue', font_weight='bold', edge_color=edge_weights, edge_cmap=plt.cm.Blues, width=edge_weights)\n",
        "# # plt.show()\n",
        "\n",
        "# # Optional: Visualize the communities\n",
        "\n",
        "# # Assign a color to each community\n",
        "# colors = [infomap.get_modules()[mapping[node]] for node in G_trim.nodes()]\n",
        "\n",
        "# # Draw the graph\n",
        "# pos = nx.spring_layout(G_trim)\n",
        "# nx.draw_networkx_nodes(G_trim, pos, node_color=colors, cmap=plt.cm.tab20, node_size=\".\")\n",
        "# nx.draw_networkx_edges(G_trim, pos, alpha=0.5)\n",
        "# plt.show()"
      ],
      "id": "Infomap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Flow analysis (NOT RUN)\n",
        "# DIDNT WORK BECAUSE THERE IS NO VARIABILITY IN THE CAPACITY\n",
        "# from networkx.algorithms.flow import maximum_flow\n",
        "\n",
        "# # Assume G is your directed graph (DiGraph)\n",
        "# source_node = \"PFNA\"\n",
        "# target_node = \"C0011881\"\n",
        "\n",
        "# # Set a default capacity for all edges if not already present\n",
        "# for u, v in G.edges():\n",
        "#     if 'capacity' not in G[u][v]:\n",
        "#         G[u][v]['capacity'] = 10  # Set to 1 or any reasonable default value\n",
        "\n",
        "# # Calculate the original maximum flow\n",
        "# original_flow_value, original_flow_dict = maximum_flow(G, source_node, target_node)\n",
        "# print(f\"Original maximum flow from {source_node} to {target_node}: {original_flow_value}\")\n",
        "\n",
        "# # Dictionary to store the impact of each node on the maximum flow\n",
        "# node_impact = {}\n",
        "\n",
        "# # Perform leave-one-out analysis\n",
        "# for node in G.nodes:\n",
        "#     if node in [source_node, target_node]:\n",
        "#         continue  # Skip source and target nodes\n",
        "\n",
        "#     # Create a copy of the graph and remove the current node\n",
        "#     G_temp = G.copy()\n",
        "#     G_temp.remove_node(node)\n",
        "\n",
        "#     # Recalculate the maximum flow\n",
        "#     flow_value, flow_dict = maximum_flow(G_temp, source_node, target_node)\n",
        "\n",
        "#     # Calculate the difference in maximum flow\n",
        "#     flow_difference = original_flow_value - flow_value\n",
        "#     node_impact[node] = flow_difference\n",
        "\n",
        "# # Identify the nodes with the most significant impact on flow\n",
        "# most_impactful_nodes = sorted(node_impact.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# # Display the nodes and their impact\n",
        "# print(\"Node impact on maximum flow (sorted):\")\n",
        "# for node, impact in most_impactful_nodes:\n",
        "#     print(f\"Node: {node}, Impact: {impact}\")"
      ],
      "id": "Flow-analysis-NOT-RUN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Node2vec analysis\n",
        "## 5.a. Node2vec \n",
        "One Note: Node2vec is for undirected graphs"
      ],
      "id": "d608e3ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Node2vec\n",
        "# Takes ~1 minute\n",
        "from node2vec import Node2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Generate Node2Vec model\n",
        "node2vec = Node2Vec(G_trim, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
        "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "\n",
        "# Extract embeddings for each node\n",
        "embeddings = model.wv\n",
        "node_ids = list(G_trim.nodes)\n",
        "node_embeddings = [embeddings[node] for node in node_ids]"
      ],
      "id": "Node2vec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.b. TSNE/UMAP on node2vec results\n"
      ],
      "id": "5a66fad1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: TSNE on node2vec\n",
        "# Run time ~ 10 seconds\n",
        "# Perform t-SNE to reduce dimensionality to 2D for visualization\n",
        "node_embeddings = np.array(node_embeddings)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_embeddings_2d = tsne.fit_transform(node_embeddings)\n",
        "\n",
        "# Note- Also tried umap, but it didn't look good"
      ],
      "id": "TSNE-on-node2vec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.c. Cluster TSNE results\n"
      ],
      "id": "50ba8592"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Cluster TSNE\n",
        "# Run time ~ 20 seconds\n",
        "from sklearn.cluster import KMeans, DBSCAN, HDBSCAN, AgglomerativeClustering, Birch\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Perform K-Means clustering on the original node embeddings\n",
        "n_clusters = 5  # Set the number of clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans_clusters = kmeans.fit_predict(node_embeddings)\n",
        "\n",
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
        "dbscan_clusters = dbscan.fit_predict(node_embeddings)\n",
        "\n",
        "# HDBSCAN clustering\n",
        "hdbscan = HDBSCAN(min_cluster_size= 8)\n",
        "hdbscan_clusters = hdbscan.fit_predict(node_embeddings)\n",
        "\n",
        "# Agglomerative Clustering\n",
        "agglomerativeclustering = AgglomerativeClustering(n_clusters= 5)\n",
        "agglomerative_clusters = agglomerativeclustering.fit_predict(node_embeddings)\n",
        "\n",
        "# Birch Clustering\n",
        "birch = Birch(n_clusters= 4)\n",
        "birch_clusters = birch.fit_predict(node_embeddings)\n",
        "\n",
        "# Gaussian Mixture Model clustering\n",
        "gmm = GaussianMixture(n_components=3, random_state=42)\n",
        "gmm_clusters = gmm.fit_predict(node_embeddings)\n",
        "\n",
        "print('K-means: ' + str(Counter(kmeans_clusters)))\n",
        "print('dbscan: '  + str(Counter(dbscan_clusters)))\n",
        "print('hdbscan: ' + str(Counter(hdbscan_clusters)))\n",
        "print('agglomerativeclustering: ' + str(Counter(agglomerative_clusters)))\n",
        "print('Birch: ' + str(Counter(birch_clusters)))\n",
        "print('GaussianMixture: ' + str(Counter(gmm_clusters)))"
      ],
      "id": "Cluster-TSNE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.d. Create Dataframe of results"
      ],
      "id": "64d62a9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Create DF of cluster results\n",
        "# Create a DataFrame linking node names with their 2D embeddings\n",
        "df_embeddings = pd.DataFrame(tsne_embeddings_2d, columns=['tsne_dim1', 'tsne_dim2'])\n",
        "# umap_embeddings = pd.DataFrame(umap_embeddings_2d, columns=['umap_dim1', 'umap_dim2'])\n",
        "# df_embeddings = pd.merge(umap_embeddings, tsne_embeddings,  left_index=True, right_index=True)\n",
        "df_embeddings['gene'] = node_ids\n",
        "\n",
        "# Reorder columns to have 'Node' first\n",
        "df_embeddings = df_embeddings[['gene', 'tsne_dim1', 'tsne_dim2']] # , 'umap_dim1', 'umap_dim2'\n",
        "\n",
        "# Merge with clusters \n",
        "df_embeddings['kmeans_clusters']            = kmeans_clusters\n",
        "df_embeddings['dbscan_clusters']            = dbscan_clusters\n",
        "df_embeddings['hdbscan_clusters']           = hdbscan_clusters\n",
        "df_embeddings['agglomerative_clusters']     = agglomerative_clusters\n",
        "df_embeddings['birch']                      = birch_clusters\n",
        "df_embeddings['gmm_clusters']               = gmm_clusters\n",
        "df_embeddings['leidenalg_community']        = leidenalg_community\n",
        "df_embeddings['leidenalg_community_min']    = leidenalg_community_min\n",
        "\n",
        "\n",
        "# Before merge: record the number of rows in each dataframe\n",
        "rows_before = {'df_embeddings': len(df_embeddings), 'gene_data': len(gene_data)}\n",
        "\n",
        "# Merging with gene_data metadata, by 'gene' and 'EntrezGeneSymbol' columns\n",
        "df_merged = pd.merge(df_embeddings, gene_data, how='left', left_on='gene', right_on='EntrezGeneSymbol')\n",
        "\n",
        "# After merge: record the number of rows in the merged dataframe\n",
        "rows_after = len(df_merged)\n",
        "\n",
        "# Print the merge details\n",
        "print(f\"Number of rows in df_embeddings before merge: {rows_before['df_embeddings']}\")\n",
        "print(f\"Number of rows in gene_data before merge: {rows_before['gene_data']}\")\n",
        "print(f\"Number of rows in df_merged after merge: {rows_after}\")\n",
        "print(f\"Number of rows matched: {rows_after}\")\n",
        "print(f\"Number of rows lost from df_embeddings: {rows_before['df_embeddings'] - rows_after}\")\n",
        "print(f\"Number of rows lost from original gene_data: {rows_before['gene_data'] - rows_after}\")\n",
        "# print(df_merged.columns)\n",
        "\n",
        "df_merged.to_csv(dir_res / 'network_graph_analysis.csv', index=False)"
      ],
      "id": "Create-DF-of-cluster-results",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.c. Plot TSNE Results\n"
      ],
      "id": "48a39c57"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: plot\n",
        "# Assign numeric codes to categories for coloring\n",
        "categories      = pd.Categorical(df_merged['sig_overall'])\n",
        "category_colors = pd.Categorical(df_merged['sig_overall']).codes\n",
        "\n",
        "# List of cluster columns to plot\n",
        "cluster_columns = [\n",
        "    'kmeans_clusters', \n",
        "    'dbscan_clusters', \n",
        "    'hdbscan_clusters', \n",
        "    'agglomerative_clusters', \n",
        "    'birch', \n",
        "    'gmm_clusters', \n",
        "    'leidenalg_community',\n",
        "    'leidenalg_community_min'\n",
        "]\n",
        "\n",
        "# Set up the figure and axes for the subplots\n",
        "fig, axes = plt.subplots(1, 8, figsize=(24, 4), sharex=True, sharey=True)\n",
        "\n",
        "# Iterate over the cluster columns and axes to create scatter plots\n",
        "for ax, column in zip(axes, cluster_columns):\n",
        "    scatter = ax.scatter(df_merged['tsne_dim1'], df_merged['tsne_dim2'], c=df_merged[column], cmap='viridis', alpha=0.6, s=10)\n",
        "    ax.set_title(column)\n",
        "    ax.set_xlabel('t-SNE Dimension 1')\n",
        "    ax.set_ylabel('t-SNE Dimension 2')\n",
        "\n",
        "# Add a color bar to the right of the plots\n",
        "# cbar = fig.colorbar(scatter, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
        "# cbar.set_label('Cluster Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get Cross tabs of the results"
      ],
      "id": "5efa0993"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Cross tabs\n",
        "cross_table = pd.crosstab(df_merged['sig_overall'], df_merged['leidenalg_community_min'])\n",
        "# Convert counts to percentages\n",
        "# cross_table_percent = cross_table.div(cross_table.sum(axis=1), axis=0) * 100\n",
        "cross_table"
      ],
      "id": "Cross-tabs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Save Graph"
      ],
      "id": "80cce106"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: save graph\n",
        "# Merge G_trim and df_merged \n",
        "temp_df = df_merged.copy()\n",
        "G_fin = G_trim.copy()\n",
        "\n",
        "# Set the 'gene' column as the index for easy lookup\n",
        "df_merged_fin = temp_df.loc[:, 'tsne_dim1':'leidenalg_community_min']\n",
        "df_merged_fin['gene'] = temp_df['EntrezGeneSymbol']\n",
        "# df_merged_fin['gene'] = temp_df['gene']\n",
        "df_merged_fin.set_index('gene', inplace=True)\n",
        "\n",
        "# Iterate through the nodes in G_trim and add attributes from df_merged\n",
        "for node in G_fin.nodes():\n",
        "    if node in df_merged.index:\n",
        "        for attr in df_merged.columns:\n",
        "            G_trim.nodes[node][attr] = df_merged.loc[node, attr]\n",
        "\n",
        "# # Example: Print node and its attributes\n",
        "# for node in G_fin.nodes(data=True):\n",
        "#     print(node)\n",
        "\n",
        "# # Save file\n",
        "# file_path = dir_res / 'ComptoxAI' / 'PFAS_prot_in_vitro_sig_fdr_trimmed_073024.graphml'\n",
        "file_path = dir_res / 'ComptoxAI' / 'PFAS_prot_in_vitro_sig_100924.graphml'\n",
        "\n",
        "# Write the graph to a GraphML file\n",
        "nx.write_graphml(G_fin, file_path)\n",
        "print(f\"Network saved to {file_path}\")"
      ],
      "id": "save-graph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OLD CODE NOT RUN\n",
        "\n",
        "## 1.a. (OLD) Identify all nodes linking PFNA, genes, and the outcome \n"
      ],
      "id": "bb437c30"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # Create cypher query\n",
        "# # Start and end strings\n",
        "# start_string = \"MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (end:Gene) WHERE end.geneSymbol IN \"\n",
        "\n",
        "# middle_string = \"CALL apoc.path.spanningTree(d, {relationshipFilter: '<GENEASSOCIATESWITHDISEASE|GENEINTERACTSWITHGENE', minLevel: 1, maxLevel: 5, endNodes: end}) YIELD path WITH nodes(path) as n RETURN n UNION MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (end:Gene) WHERE end.geneSymbol IN\"\n",
        "\n",
        "# end_string = \"CALL apoc.path.spanningTree(c, {relationshipFilter: 'CHEMICALINCREASESEXPRESSION>|CHEMICALDECREASESEXPRESSION>|GENEINTERACTSWITHGENE>', minLevel: 1, maxLevel: 7, endNodes: end}) YIELD path WITH nodes(path) as n RETURN n;\"\n",
        "\n",
        "# # Combine the start and end strings with the unique_gene_symbols_list\n",
        "# first_query = start_string + Entrez_Genes + middle_string + Entrez_Genes + end_string\n",
        "\n",
        "# # Run first query to identify all nodes\n",
        "# nodes = db.run_cypher(first_query) "
      ],
      "id": "7da216bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # Unlist everything and get a list of separate dictionaries\n",
        "# flattened_list = [inner_dict for outer_dict in nodes for inner_dict in outer_dict['n']]\n",
        "\n",
        "# # Remove duplicates by converting to a set and back to a list\n",
        "# unique_list = list({tuple(item.items()) for item in flattened_list})\n",
        "\n",
        "# # Convert the list of tuples back to dictionaries\n",
        "# unique_list = [dict(item) for item in unique_list]\n",
        "\n",
        "# print('The number of unique nodes, including the PFAS and disease node, is ' +  str(len(unique_list)))\n",
        "# # Look at first few elements of list\n",
        "# # pprint(unique_list[0:2])"
      ],
      "id": "79afff61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # Extract geneSymbol\n",
        "# # Create a set to store unique geneSymbol values\n",
        "# unique_gene_symbols = set()\n",
        "\n",
        "# # Iterate through the list and collect unique geneSymbol values\n",
        "# for dictionary in flattened_list:\n",
        "#     if 'geneSymbol' in dictionary:\n",
        "#         unique_gene_symbols.add(dictionary['geneSymbol'])\n",
        "\n",
        "# # Convert the set back to a list if needed\n",
        "# unique_gene_symbols_list = list(unique_gene_symbols)\n",
        "\n",
        "# # Now, 'unique_gene_symbols_list' contains all unique geneSymbol values\n",
        "\n",
        "# print('The number of unique genes is ' +  str(len(unique_gene_symbols_list)) + \", and \" + str(len(set(unique_gene_symbols_list) & set(Entrez_Gene_Symbols))) + \" of these genes are from the original data.\")"
      ],
      "id": "1a88f672",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.b. (OLD) With all nodes, identify all of the relationships\n"
      ],
      "id": "a01c4c3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # Create cypher query\n",
        "# # Start and end strings\n",
        "# start_string = \"MATCH (c:Chemical {xrefDTXSID: 'DTXSID8031863'}) MATCH (d:Disease {commonName: 'Diabetic Nephropathy'}) MATCH (node1:Gene) WHERE node1.geneSymbol IN \"\n",
        "\n",
        "# end_string = \"WITH collect(id(node1))+collect(c)+collect(d) as nodes CALL apoc.algo.cover(nodes) YIELD rel RETURN  startNode(rel), rel, endNode(rel);\"\n",
        "\n",
        "# # Combine the start and end strings with the unique_gene_symbols_list\n",
        "# query_string = start_string + \"[\" + \", \".join([\"'\" + symbol + \"'\" for symbol in unique_gene_symbols_list]) + \"]\" + end_string\n",
        "\n",
        "# # Run Cypher Query\n",
        "# data = db.run_cypher(query_string)"
      ],
      "id": "d1659d7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.b Add PFAS and scRNAseq edges- NOT PERFORMED, BECAUSE THERE WOULD BASICIALLY BE A LINK BETWEEN EVERY NODE AND EVERY GENE"
      ],
      "id": "9c59152a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# #| label: add additional edges\n",
        "# genes_with_pfas_human    = gene_data[gene_data['p_value_em'] < 0.05]['EntrezGeneSymbol'].tolist()\n",
        "# genes_with_disease_human = gene_data[gene_data['p_value_mo'] < 0.05]['EntrezGeneSymbol'].tolist()\n",
        "# genes_with_pfas_in_vitro = gene_data[gene_data['sc_gene'] == 'sig scRNAseq']['EntrezGeneSymbol'].tolist()\n",
        "\n",
        "# # not altered from here... \n",
        "# unique_sc_data = list(Counter(sc_data.gene).keys())\n",
        "# unique_sc_data = pd.DataFrame(unique_sc_data, columns= ['gene'])\n",
        "\n",
        "# # Create a new column to store whether the edge exists or not\n",
        "# unique_sc_data['edge_exists'] = False\n",
        "\n",
        "# # Check if edge exists\n",
        "# for idx, row in unique_sc_data.iterrows():\n",
        "#     sc_gene = row['gene']  # The SNP node\n",
        "#     exposure = chemical_common_name  # The name of the PFAS node (which is already be in the network)\n",
        "\n",
        "#     # Check if there is an edge between chemical_common_name and sc_gene\n",
        "#     if G.has_edge(exposure, sc_gene):\n",
        "#         unique_sc_data.at[idx, 'edge_exists'] = True\n",
        "\n",
        "\n",
        "# # Count how many True values there are in the 'edge_exists' column\n",
        "# true_count = unique_sc_data['edge_exists'].sum()\n",
        "\n",
        "# # Create a summary table\n",
        "# summary_table = pd.DataFrame({\n",
        "#     'Total Genes': [len(unique_sc_data)],\n",
        "#     'Edges Found (True)': [true_count],\n",
        "#     'Edges Not Found (False)': [len(unique_sc_data) - true_count]\n",
        "# })\n",
        "\n",
        "# # Display the summary table\n",
        "# print(summary_table)\n",
        "\n",
        "# # Add edge between exposure and scRNAseq genes\n",
        "# # Add or modify edge between exposure and scRNAseq genes\n",
        "# for idx, row in unique_sc_data.iterrows():\n",
        "#     sc_gene = row['gene']  # The gene node\n",
        "#     exposure = chemical_common_name  # The name of the PFAS node\n",
        "\n",
        "#     # Check if the gene is in the graph\n",
        "#     if sc_gene not in G.nodes:\n",
        "#         print(f\"Gene {sc_gene} is not in the graph!\")\n",
        "#         continue  # Skip this gene if it isn't present\n",
        "\n",
        "#     # Check if there is already an edge between exposure and sc_gene\n",
        "#     if G.has_edge(exposure, sc_gene):\n",
        "#         # If the edge exists, modify the relationship and the weight\n",
        "#         G[exposure][sc_gene]['relationship'] = 'ComptoxAI and Organoid association'\n",
        "#         G[exposure][sc_gene]['weight'] = 2\n",
        "#         print(f\"Modified edge {idx}: exposure={exposure}, sc_gene={sc_gene}, relationship='ComptoxAI and Organoid association', weight=2\")\n",
        "#     else:\n",
        "#         # If the edge doesn't exist, add it with a different relationship and weight\n",
        "#         G.add_edge(exposure, sc_gene, relationship='Organoid association', weight=1)\n",
        "#         print(f\"Added new edge {idx}: exposure={exposure}, sc_gene={sc_gene}, relationship='Organoid association', weight=1\")"
      ],
      "id": "176e83fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GraphSAGE model (not relevant to this analysis)"
      ],
      "id": "47650b00"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: GraphSAGE- NOT RUN\n",
        "# # The graph needs to have the same attributes for this algorithm\n",
        "# def get_all_attributes(G_trim):\n",
        "#     node_attributes = set()\n",
        "#     edge_attributes = set()\n",
        "\n",
        "#     # Collect node attributes\n",
        "#     for node, attr in G_trim.nodes(data=True):\n",
        "#         node_attributes.update(attr.keys())\n",
        "\n",
        "#     # Collect edge attributes\n",
        "#     for u, v, attr in G_trim.edges(data=True):\n",
        "#         edge_attributes.update(attr.keys())\n",
        "\n",
        "#     return node_attributes, edge_attributes\n",
        "\n",
        "# # Get unique attributes\n",
        "# node_attrs, edge_attrs = get_all_attributes(G)\n",
        "# print(f\"Unique node attributes: {node_attrs}\")\n",
        "# print(f\"Unique edge attributes: {edge_attrs}\")\n",
        "\n",
        "# # Desired attributes to keep\n",
        "# desired_attributes = {'commonName', 'type', 'typeOfGene'}\n",
        "\n",
        "# # Make copy of data\n",
        "# G_reduced = G_trim.copy()\n",
        "\n",
        "# # Function to filter node attributes\n",
        "# def filter_node_attributes(G_reduced, desired_attributes):\n",
        "#     for node in G_reduced.nodes:\n",
        "#         current_attributes = G_reduced.nodes[node]\n",
        "#         filtered_attributes = {k: v for k, v in current_attributes.items() if k in desired_attributes}\n",
        "#         G_reduced.nodes[node].clear()\n",
        "#         G_reduced.nodes[node].update(filtered_attributes)\n",
        "\n",
        "# # # Apply the function to filter node attributes\n",
        "# filter_node_attributes(G_reduced, desired_attributes)\n",
        "\n",
        "# # Get unique attributes\n",
        "# node_attrs, edge_attrs = get_all_attributes(G_reduced)\n",
        "# print(f\"Filtered unique node attributes: {node_attrs}\")\n",
        "# print(f\"Filtered unique edge attributes: {edge_attrs}\")\n",
        "\n",
        "# # Set values for nodes\n",
        "# G_reduced.nodes['C0011881']['typeOfGene'] = 'disease' \n",
        "# G_reduced.nodes['PFNA']['typeOfGene'] = 'Chemical'  \n",
        "\n",
        "# # Check which nodes are missing the \"typeOfGene\" attribute\n",
        "# missing_typeOfGene_nodes = [node for node, attr in G_reduced.nodes(data=True) if 'typeOfGene' not in attr]\n",
        "# print(f\"Number of missing typeOfGene: {len(missing_typeOfGene_nodes)}\")\n",
        "\n",
        "# # Check which nodes are missing the \"typeOfGene\" attribute\n",
        "# missing_typeOfGene_nodes = [node for node, attr in G_reduced.nodes(data=True) if 'type' not in attr]\n",
        "# print(f\"Number of missing type: {len(missing_typeOfGene_nodes)}\")"
      ],
      "id": "GraphSAGE--NOT-RUN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: GraphSage fit- Not Run\n",
        "import torch\n",
        "# import torch.nn.functional as F\n",
        "# from torch_geometric.nn import SAGEConv\n",
        "# from torch_geometric.data import Data\n",
        "\n",
        "# # Example graph data (this should be replaced with your actual graph data)\n",
        "# # Assume G is a networkx directed graph with node features and labels\n",
        "\n",
        "# # Convert NetworkX graph to PyTorch Geometric Data\n",
        "# from torch_geometric.utils import from_networkx\n",
        "\n",
        "# # Create PyTorch Geometric data from NetworkX graph\n",
        "# data = from_networkx(G_reduced)\n",
        "\n",
        "# # Ensure that data has node features (x), labels (y), and edge index\n",
        "# if data.x is None or data.y is None:\n",
        "#     raise ValueError(\"The input graph does not contain node features and/or labels.\")\n",
        "\n",
        "# # Defining a simple GraphSAGE Model\n",
        "# class GraphSAGE(torch.nn.Module):\n",
        "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "#         super(GraphSAGE, self).__init__()\n",
        "#         self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "#         self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         # First layer\n",
        "#         x = self.conv1(x, edge_index)\n",
        "#         x = F.relu(x)  # Apply ReLU activation\n",
        "#         # Second layer\n",
        "#         x = self.conv2(x, edge_index)\n",
        "#         return F.log_softmax(x, dim=1)  # Apply log_softmax for classification\n",
        "\n",
        "# # Set the parameters for the model\n",
        "# in_channels = data.x.shape[1]\n",
        "# hidden_channels = 32\n",
        "# out_channels = len(torch.unique(data.y))  # Number of unique classes in the labels\n",
        "\n",
        "# # Instantiate the model\n",
        "# model = GraphSAGE(in_channels, hidden_channels, out_channels)\n",
        "\n",
        "# # Define optimizer\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "# # Create masks for splitting data into training, validation, and test sets\n",
        "# data.train_mask = torch.rand(data.num_nodes) < 1  # Randomly assign 80% of nodes to training set\n",
        "# data.test_mask = ~data.train_mask  # The rest are used as a test set\n",
        "\n",
        "# # Define the training loop\n",
        "# def train():\n",
        "#     model.train()  # Set the model to training mode\n",
        "#     optimizer.zero_grad()  # Zero out the gradients from the previous step\n",
        "#     out = model(data.x, data.edge_index)  # Forward pass through the model\n",
        "#     loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])  # Calculate the loss on training nodes\n",
        "#     loss.backward()  # Backward pass (compute gradients)\n",
        "#     optimizer.step()  # Update the model weights\n",
        "#     return loss\n",
        "\n",
        "# # Training loop\n",
        "# epochs = 200\n",
        "# for epoch in range(epochs):\n",
        "#     loss = train()\n",
        "#     if epoch % 10 == 0:\n",
        "#         print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# # Evaluating the model\n",
        "# def test():\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "#     out = model(data.x, data.edge_index)  # Get model predictions\n",
        "#     pred = out.argmax(dim=1)  # Get predicted classes\n",
        "#     correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()  # Count correct predictions\n",
        "#     acc = int(correct) / int(data.test_mask.sum())  # Calculate accuracy\n",
        "#     return acc\n",
        "\n",
        "# # Testing the model\n",
        "# accuracy = test()\n",
        "# print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "id": "GraphSage-fit--Not-Run",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/jagoodri/.pyenv/versions/3.10.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}